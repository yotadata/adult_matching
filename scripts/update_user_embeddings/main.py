#!/usr/bin/env python3
"""Incrementally upsert user embeddings into Supabase."""

import argparse
import json
import os
import socket
import sys
import uuid
from datetime import datetime, timezone
from functools import lru_cache
from pathlib import Path
from typing import Iterable, List, Sequence
from urllib.parse import parse_qs, urlencode, urlsplit, urlunsplit, quote

import numpy as np
import pandas as pd
import psycopg
from pgvector.psycopg import register_vector


@lru_cache(maxsize=1)
def _resolve_project_ref() -> str | None:
    project_id = os.environ.get("SUPABASE_PROJECT_ID")
    if project_id:
        return project_id
    legacy = os.environ.get("SUPABASE_PROJECT_REF")
    if legacy:
        print(
            json.dumps(
                {
                    "warn": "deprecated_env_var",
                    "details": "Set SUPABASE_PROJECT_ID; SUPABASE_PROJECT_REF is deprecated.",
                },
                ensure_ascii=False,
            ),
            file=sys.stderr,
        )
        return legacy
    return None


def _ensure_ipv4_hostaddr(conninfo: str, allow_pooler: bool = True) -> str:
    parsed = urlsplit(conninfo)
    if parsed.scheme not in ("postgresql", "postgres", "postgresql+psycopg"):
        return conninfo

    query = parse_qs(parsed.query)
    disable_ssl = os.environ.get("SUPABASE_DB_DISABLE_SSL") == "1"
    desired_sslmode = os.environ.get("PGSSLMODE")
    if disable_ssl:
        desired_sslmode = desired_sslmode or None
    else:
        desired_sslmode = desired_sslmode or "require"
    if desired_sslmode and "sslmode" not in query:
        query["sslmode"] = [desired_sslmode]
    if not disable_ssl and "sslrootcert" not in query and os.environ.get("PGSSLROOTCERT"):
        query["sslrootcert"] = [os.environ["PGSSLROOTCERT"]]
    if query.get("hostaddr"):
        return urlunsplit((parsed.scheme, parsed.netloc, parsed.path, urlencode(query, doseq=True), parsed.fragment))

    host = parsed.hostname
    port = parsed.port or 5432
    if not host:
        return conninfo

    ipv4_addr = None
    try:
        for family, _, _, _, sockaddr in socket.getaddrinfo(host, port):
            if family == socket.AF_INET:
                ipv4_addr = sockaddr[0]
                break
    except socket.gaierror:
        ipv4_addr = None

    if allow_pooler and not ipv4_addr:
        project_ref = _resolve_project_ref()
        if not project_ref:
            parts = parsed.hostname.split(".") if parsed.hostname else []
            if len(parts) >= 3 and parts[0] == "db":
                project_ref = parts[1]
        pooler_host = os.environ.get("SUPABASE_POOLER_HOST")
        if not pooler_host:
            region = os.environ.get("SUPABASE_REGION")
            if region:
                pooler_host = f"{region}.pooler.supabase.com"
        if pooler_host:
            pooler_port = os.environ.get("SUPABASE_POOLER_PORT") or "6543"
            username = parsed.username or ""
            password = parsed.password or ""
            if project_ref and username and "." not in username:
                username = f"{username}.{project_ref}"
            auth = quote(username)
            if password:
                auth = f"{auth}:{quote(password)}"
            netloc = f"{auth}@{pooler_host}:{pooler_port}"
            pooler_conn = urlunsplit((parsed.scheme, netloc, parsed.path, parsed.query, parsed.fragment))
            print(json.dumps({"info": "pooler_url_computed", "url": pooler_conn}, ensure_ascii=False))
            return _ensure_ipv4_hostaddr(pooler_conn, allow_pooler=False)

    if not ipv4_addr:
        return urlunsplit((parsed.scheme, parsed.netloc, parsed.path, urlencode(query, doseq=True), parsed.fragment))

    query.setdefault("hostaddr", []).append(ipv4_addr)
    return urlunsplit((parsed.scheme, parsed.netloc, parsed.path, urlencode(query, doseq=True), parsed.fragment))


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Upsert user embeddings generated by gen_user_embeddings.py")
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=Path("ml/artifacts/live/user_embeddings"),
        help="Directory containing user_embeddings parquet output",
    )
    parser.add_argument(
        "--parquet-name",
        type=str,
        default="user_embeddings.parquet",
        help="Parquet filename inside artifacts dir",
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=500,
        help="Number of rows to upsert per batch",
    )
    parser.add_argument(
        "--db-url",
        type=str,
        default=None,
        help="Postgres connection string (defaults to SUPABASE_DB_URL or REMOTE_DATABASE_URL)",
    )
    parser.add_argument(
        "--model-version",
        type=str,
        default=None,
        help="Override model_version when parquet is missing the column",
    )
    parser.add_argument("--dry-run", action="store_true", help="Report counts without writing to the database")
    return parser.parse_args()


def _normalize_embedding(value: object) -> List[float]:
    if value is None:
        raise ValueError("embedding is None")
    if isinstance(value, list):
        return [float(x) for x in value]
    if isinstance(value, np.ndarray):
        return value.astype(float).tolist()
    if hasattr(value, "tolist"):
        data = value.tolist()
        if isinstance(data, list):
            return [float(x) for x in data]
    if isinstance(value, (tuple, Sequence)):
        return [float(x) for x in value]
    raise ValueError(f"Unsupported embedding type: {type(value)}")


def load_embeddings(parquet_path: Path) -> pd.DataFrame:
    if not parquet_path.exists():
        raise FileNotFoundError(f"Parquet not found: {parquet_path}")
    df = pd.read_parquet(parquet_path)
    if "embedding" not in df.columns:
        raise ValueError(f"{parquet_path} must contain 'embedding' column")
    if "user_id" not in df.columns:
        if "reviewer_id" in df.columns:
            df = df.rename(columns={"reviewer_id": "user_id"})
        else:
            raise ValueError(f"{parquet_path} must contain 'user_id' (or 'reviewer_id') column")
    return df


def prepare_rows(df: pd.DataFrame, fallback_version: str | None) -> tuple[list[dict], int, int]:
    rows: list[dict] = []
    skipped = 0
    non_uuid = 0
    for record in df.itertuples(index=False):
        try:
            raw_user = getattr(record, "user_id")
            user_id = uuid.UUID(str(raw_user))
        except Exception:
            non_uuid += 1
            continue
        embedding = getattr(record, "embedding", None)
        if embedding is None:
            skipped += 1
            continue
        try:
            vector = _normalize_embedding(embedding)
        except ValueError:
            skipped += 1
            continue
        version = getattr(record, "model_version", None)
        if version is None or str(version).strip() == "":
            version = fallback_version or "unknown"
        rows.append({"user_id": user_id, "embedding": vector, "model_version": str(version)})
    return rows, skipped, non_uuid


def chunked(items: List[dict], size: int) -> Iterable[List[dict]]:
    for idx in range(0, len(items), size):
        yield items[idx : idx + size]


def upsert_user_embeddings(conn: psycopg.Connection, rows: List[dict], chunk_size: int) -> int:
    sql = """
        INSERT INTO public.user_embeddings (user_id, embedding, model_version, updated_at)
        VALUES (%(user_id)s, %(embedding)s, %(model_version)s, now())
        ON CONFLICT (user_id) DO UPDATE
        SET embedding = EXCLUDED.embedding,
            model_version = EXCLUDED.model_version,
            updated_at = now()
    """
    total = 0
    with conn.cursor() as cur:
        for chunk in chunked(rows, chunk_size):
            cur.executemany(sql, chunk)
            total += len(chunk)
    return total


def main() -> None:
    args = parse_args()
    artifacts_dir = args.artifacts_dir.resolve()
    parquet_path = artifacts_dir / args.parquet_name

    df = load_embeddings(parquet_path)
    fallback_version = args.model_version
    rows, skipped, non_uuid = prepare_rows(df, fallback_version)
    if not rows:
        print(
            json.dumps(
                {
                    "event": "no_rows_to_upsert",
                    "parquet": str(parquet_path),
                    "skipped": skipped,
                    "non_uuid": non_uuid,
                },
                ensure_ascii=False,
            )
        )
        return

    db_url = args.db_url or os.environ.get("SUPABASE_DB_URL") or os.environ.get("REMOTE_DATABASE_URL")
    if not db_url:
        raise ValueError("Database URL not provided. Set --db-url or SUPABASE_DB_URL / REMOTE_DATABASE_URL env.")
    db_url = _ensure_ipv4_hostaddr(db_url)

    summary = {
        "event": "user_embedding_batch",
        "rows": len(rows),
        "skipped": skipped,
        "non_uuid": non_uuid,
        "model_version": rows[0].get("model_version"),
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "artifacts_dir": str(artifacts_dir),
    }
    print(json.dumps(summary, ensure_ascii=False))

    if args.dry_run:
        return

    with psycopg.connect(db_url, autocommit=False) as conn:
        register_vector(conn)
        inserted = upsert_user_embeddings(conn, rows, args.chunk_size)
        conn.commit()

    print(
        json.dumps(
            {
                "event": "user_embeddings_upserted",
                "inserted": inserted,
                "chunk_size": args.chunk_size,
            },
            ensure_ascii=False,
        )
    )


if __name__ == "__main__":
    main()
