# ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ä»•æ§˜æ›¸

**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: ã‚¢ãƒ€ãƒ«ãƒˆå‹•ç”»ãƒãƒƒãƒãƒ³ã‚°ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³  
**æ–‡æ›¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v1.0  
**ä½œæˆæ—¥**: 2025å¹´9æœˆ3æ—¥  
**ç›®çš„**: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ´»ç”¨ã—ãŸåˆæœŸå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆ

---

## ğŸ“‹ ç›®æ¬¡

1. [èª²é¡Œã¨è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ](#èª²é¡Œã¨è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)
2. [ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿åˆ†æ](#ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿åˆ†æ) 
3. [ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿæˆæˆ¦ç•¥](#ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿæˆæˆ¦ç•¥)
4. [å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å¤‰æ›è¨­è¨ˆ](#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å¤‰æ›è¨­è¨ˆ)
5. [MLå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](#mlå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
6. [å“è³ªä¿è¨¼ãƒ»æ¤œè¨¼](#å“è³ªä¿è¨¼æ¤œè¨¼)
7. [å®Ÿè£…è¨ˆç”»](#å®Ÿè£…è¨ˆç”»)

---

## ğŸ¯ èª²é¡Œã¨è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### ç¾çŠ¶ã®èª²é¡Œ
**å†·é–‹å§‹å•é¡Œ (Cold Start Problem)**
- æ–°ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„
- Two-Towerãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ã¯å¤§é‡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼-ã‚¢ã‚¤ãƒ†ãƒ ç›¸äº’ä½œç”¨ãŒå¿…è¦
- å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿åé›†ã«ã¯æ•°ãƒ¶æœˆã€œæ•°å¹´ãŒå¿…è¦

### è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
**ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å­¦ç¿’**
1. **ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ â†’ ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å¤‰æ›**: ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã‚’ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã—ã¦æ‰±ã„
2. **è©•ä¾¡å€¤ â†’ ã„ã„ã­è¡Œå‹•å¤‰æ›**: è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ Like/Skip è¡Œå‹•ã«å¤‰æ›
3. **ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½å¤‰æ›**: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’æŠ½å‡º
4. **å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« â†’ å®Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼é©å¿œ**: å®Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### æœŸå¾…åŠ¹æœ
- âœ… **å³åº§ã®é‹ç”¨é–‹å§‹**: ã‚¢ãƒ—ãƒªãƒªãƒªãƒ¼ã‚¹æ™‚ç‚¹ã§æ¨è–¦æ©Ÿèƒ½æä¾›
- âœ… **å¤šæ§˜ãªå—œå¥½å­¦ç¿’**: æ§˜ã€…ãªãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®å¥½ã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
- âœ… **å“è³ªã®äº‹å‰æ¤œè¨¼**: å®Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼å‰ã«ãƒ¢ãƒ‡ãƒ«å“è³ªã‚’æ¤œè¨¼
- âœ… **æ®µéšçš„æ”¹å–„**: å®Ÿãƒ‡ãƒ¼ã‚¿ã§ç¶™ç¶šçš„ã«ç²¾åº¦å‘ä¸Š

---

## ğŸ“Š ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿åˆ†æ

### ç¾åœ¨å–å¾—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
```json
{
  "total_reviews": 21,
  "valid_after_cleaning": 3,
  "data_structure": {
    "review_text": "è©³ç´°ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ",
    "rating": 1.0,  // æ•°å€¤è©•ä¾¡ï¼ˆä¸€éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼‰
    "element_info": {...}  // HTMLæ§‹é€ æƒ…å ±
  }
}
```

### ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´åˆ†æ

#### ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
```python
# data_processing/analysis/review_analysis.py

class ReviewDataAnalyzer:
    def analyze_review_quality(self, reviews):
        return {
            'text_length_distribution': self.analyze_text_lengths(reviews),
            'sentiment_distribution': self.analyze_sentiments(reviews), 
            'rating_distribution': self.analyze_ratings(reviews),
            'reviewer_diversity': self.analyze_reviewer_patterns(reviews),
            'content_coverage': self.analyze_content_coverage(reviews)
        }
```

#### æ‹¡å¼µãƒ‡ãƒ¼ã‚¿åé›†è¦ä»¶
```yaml
target_data_volume:
  minimum_reviews: 100000  # æœ€ä½10ä¸‡ä»¶
  minimum_videos: 10000    # æœ€ä½1ä¸‡å‹•ç”»
  minimum_reviewers: 5000  # æœ€ä½5åƒãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼

data_quality_requirements:
  text_length_min: 20      # 20æ–‡å­—ä»¥ä¸Šã®ãƒ¬ãƒ“ãƒ¥ãƒ¼
  text_length_max: 5000    # 5000æ–‡å­—ä»¥ä¸‹
  rating_coverage: 0.3     # 30%ä»¥ä¸Šã«è©•ä¾¡å€¤ã‚ã‚Š
  reviewer_min_reviews: 3  # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã‚ãŸã‚Šæœ€ä½3ä»¶

data_diversity_requirements:
  genre_coverage: 0.8      # 80%ä»¥ä¸Šã®ã‚¸ãƒ£ãƒ³ãƒ«ã‚«ãƒãƒ¼
  rating_variance: 1.5     # è©•ä¾¡å€¤ã®åˆ†æ•£1.5ä»¥ä¸Š
  temporal_span: 12months  # 12ãƒ¶æœˆé–“ã®ãƒ‡ãƒ¼ã‚¿
```

---

## ğŸ‘¤ ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿæˆæˆ¦ç•¥

### ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ â†’ ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å¤‰æ›

#### 1. ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼IDæŠ½å‡ºãƒ»åŒ¿ååŒ–
```python
class PseudoUserGenerator:
    def extract_reviewers(self, reviews):
        """ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼IDã‚’æŠ½å‡º"""
        reviewers = {}
        for review in reviews:
            reviewer_id = review.get('external_reviewer_id') or \
                         self.generate_pseudo_reviewer_id(review)
            
            if reviewer_id not in reviewers:
                reviewers[reviewer_id] = {
                    'pseudo_user_id': self.anonymize_reviewer_id(reviewer_id),
                    'reviews': [],
                    'profile': self.init_user_profile()
                }
            
            reviewers[reviewer_id]['reviews'].append(review)
        
        return reviewers
    
    def anonymize_reviewer_id(self, reviewer_id):
        """ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼IDã‚’åŒ¿ååŒ–ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã«å¤‰æ›"""
        return hashlib.sha256(f"pseudo_{reviewer_id}_{self.salt}".encode()).hexdigest()
```

#### 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
```python
def generate_user_profile(self, reviewer_reviews):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼å±¥æ­´ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
    profile = {
        'user_id': reviewer['pseudo_user_id'],
        'review_count': len(reviewer_reviews),
        'avg_rating': np.mean([r.get('rating', 0) for r in reviewer_reviews]),
        'rating_variance': np.var([r.get('rating', 0) for r in reviewer_reviews]),
        'review_length_avg': np.mean([len(r.get('review_text', '')) for r in reviewer_reviews]),
        'sentiment_profile': self.analyze_sentiment_profile(reviewer_reviews),
        'genre_preferences': self.extract_genre_preferences(reviewer_reviews),
        'temporal_activity': self.analyze_temporal_activity(reviewer_reviews)
    }
    return profile
```

### ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

#### è©•ä¾¡å€¤ â†’ Like/Skip å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯
```python
class ReviewToActionConverter:
    def __init__(self):
        # è©•ä¾¡å€¤ã‹ã‚‰ã„ã„ã­ç¢ºç‡ã¸ã®å¤‰æ›ãƒ†ãƒ¼ãƒ–ãƒ«
        self.rating_to_like_probability = {
            5.0: 0.95,  # 5ã¤æ˜Ÿ â†’ 95%ã„ã„ã­
            4.0: 0.80,  # 4ã¤æ˜Ÿ â†’ 80%ã„ã„ã­  
            3.0: 0.50,  # 3ã¤æ˜Ÿ â†’ 50%ã„ã„ã­
            2.0: 0.20,  # 2ã¤æ˜Ÿ â†’ 20%ã„ã„ã­
            1.0: 0.05   # 1ã¤æ˜Ÿ â†’ 5%ã„ã„ã­
        }
    
    def convert_review_to_actions(self, review, video_id, user_id):
        """ãƒ¬ãƒ“ãƒ¥ãƒ¼1ä»¶ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›"""
        actions = []
        
        # 1. ãƒ¡ã‚¤ãƒ³ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ãŸã¨ã„ã†ã“ã¨ï¼è¦–è´ã—ãŸï¼‰
        main_action = self.generate_main_action(review, video_id, user_id)
        actions.append(main_action)
        
        # 2. é–¢é€£å‹•ç”»ã¸ã®æ¨å®šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆåŒã˜å‡ºæ¼”è€…ãƒ»ã‚¸ãƒ£ãƒ³ãƒ«ç­‰ï¼‰
        related_actions = self.generate_related_actions(review, user_id)
        actions.extend(related_actions)
        
        return actions
    
    def generate_main_action(self, review, video_id, user_id):
        """ãƒ¡ã‚¤ãƒ³ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        rating = review.get('rating', 3.0)
        sentiment_score = self.analyze_sentiment(review.get('review_text', ''))
        
        # è©•ä¾¡å€¤ã¨æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã„ã­ç¢ºç‡ã‚’æ±ºå®š
        like_prob = self.calculate_like_probability(rating, sentiment_score)
        action = 'like' if random.random() < like_prob else 'skip'
        
        return {
            'user_id': user_id,
            'video_id': video_id,
            'action': action,
            'confidence': like_prob if action == 'like' else (1 - like_prob),
            'timestamp': self.parse_review_timestamp(review),
            'source': 'review_conversion',
            'metadata': {
                'original_rating': rating,
                'sentiment_score': sentiment_score,
                'review_length': len(review.get('review_text', ''))
            }
        }
```

#### é–¢é€£å‹•ç”»ã¸ã®æ¨å®šè¡Œå‹•ç”Ÿæˆ
```python
def generate_related_actions(self, review, user_id, num_actions=5):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰é–¢é€£å‹•ç”»ã¸ã®è¡Œå‹•ã‚’æ¨å®šç”Ÿæˆ"""
    
    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords = self.extract_keywords(review['review_text'])
    preferences = self.infer_preferences(review)
    
    # é¡ä¼¼å‹•ç”»ã‚’æ¤œç´¢
    similar_videos = self.find_similar_videos(keywords, preferences)
    
    actions = []
    for video in similar_videos[:num_actions]:
        # å¥½ã¿åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç¢ºç‡ã‚’è¨ˆç®—
        preference_score = self.calculate_preference_match(preferences, video)
        action_prob = self.preference_to_action_probability(preference_score)
        
        action = {
            'user_id': user_id,
            'video_id': video['id'],
            'action': 'like' if random.random() < action_prob else 'skip',
            'confidence': action_prob,
            'timestamp': self.generate_realistic_timestamp(review),
            'source': 'inferred_preference',
            'metadata': {
                'preference_score': preference_score,
                'similarity_basis': keywords,
                'inference_confidence': 0.6  # æ¨å®šãƒ‡ãƒ¼ã‚¿ãªã®ã§ä½ã„ä¿¡é ¼åº¦
            }
        }
        actions.append(action)
    
    return actions
```

---

## ğŸ”„ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å¤‰æ›è¨­è¨ˆ

### ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
# ml_pipeline/preprocessing/review_to_training_data.py

class ReviewToTrainingDataPipeline:
    def __init__(self):
        self.user_generator = PseudoUserGenerator()
        self.action_converter = ReviewToActionConverter()
        self.feature_extractor = UserFeatureExtractor()
        self.data_augmenter = DataAugmenter()
    
    def process_reviews_to_training_data(self, reviews):
        """ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å…¨ä½“å¤‰æ›"""
        
        # Step 1: ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿæˆ
        pseudo_users = self.user_generator.extract_reviewers(reviews)
        print(f"Generated {len(pseudo_users)} pseudo users")
        
        # Step 2: ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        user_actions = []
        for user_id, user_data in pseudo_users.items():
            actions = self.action_converter.convert_user_reviews(user_data)
            user_actions.extend(actions)
        
        print(f"Generated {len(user_actions)} user actions")
        
        # Step 3: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        user_features = self.feature_extractor.extract_user_features(pseudo_users)
        item_features = self.feature_extractor.extract_item_features(reviews)
        
        # Step 4: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ»ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
        augmented_data = self.data_augmenter.augment_training_data(
            user_actions, user_features, item_features
        )
        
        # Step 5: è¨“ç·´ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›
        training_dataset = self.format_for_training(augmented_data)
        
        return training_dataset
```

### ç‰¹å¾´é‡è¨­è¨ˆ

#### ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å¾´é‡ï¼ˆç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
```python
class UserFeatureExtractor:
    def extract_user_features(self, pseudo_users):
        features = {}
        
        for user_id, user_data in pseudo_users.items():
            reviews = user_data['reviews']
            
            # åŸºæœ¬çµ±è¨ˆç‰¹å¾´é‡
            basic_features = {
                'review_count': len(reviews),
                'avg_rating': np.mean([r.get('rating', 3.0) for r in reviews]),
                'rating_std': np.std([r.get('rating', 3.0) for r in reviews]),
                'avg_review_length': np.mean([len(r.get('review_text', '')) for r in reviews])
            }
            
            # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æç‰¹å¾´é‡
            text_features = self.extract_text_features(reviews)
            
            # å—œå¥½ç‰¹å¾´é‡
            preference_features = self.extract_preference_features(reviews)
            
            # è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
            behavioral_features = self.extract_behavioral_features(reviews)
            
            features[user_id] = {
                **basic_features,
                **text_features,
                **preference_features,
                **behavioral_features
            }
        
        return features
    
    def extract_preference_features(self, reviews):
        """ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰å—œå¥½ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        
        # ã‚¸ãƒ£ãƒ³ãƒ«å—œå¥½
        genre_mentions = Counter()
        performer_mentions = Counter()
        sentiment_scores = []
        
        for review in reviews:
            text = review.get('review_text', '')
            
            # ã‚¸ãƒ£ãƒ³ãƒ«ãƒ»å‡ºæ¼”è€…ã¸ã®è¨€åŠã‚’æŠ½å‡º
            genres = self.extract_genre_mentions(text)
            performers = self.extract_performer_mentions(text)
            
            genre_mentions.update(genres)
            performer_mentions.update(performers)
            
            # æ„Ÿæƒ…åˆ†æ
            sentiment = self.analyze_sentiment(text)
            sentiment_scores.append(sentiment)
        
        return {
            'preferred_genres': dict(genre_mentions.most_common(5)),
            'preferred_performers': dict(performer_mentions.most_common(3)),
            'avg_sentiment': np.mean(sentiment_scores),
            'sentiment_variance': np.var(sentiment_scores),
            'positive_review_ratio': sum(1 for s in sentiment_scores if s > 0.1) / len(sentiment_scores)
        }
```

### ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæˆ¦ç•¥

#### 1. è² ä¾‹ç”Ÿæˆï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
```python
class NegativeSampler:
    def generate_negative_samples(self, positive_actions, ratio=2.0):
        """ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã„ã„ã­ï¼‰ã«å¯¾ã—ã¦ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰ã‚’ç”Ÿæˆ"""
        
        negative_actions = []
        
        for pos_action in positive_actions:
            if pos_action['action'] == 'like':
                # åŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å«Œã„ãã†ãªå‹•ç”»ã‚’æ¨å®š
                negative_videos = self.find_dissimilar_videos(
                    user_id=pos_action['user_id'],
                    liked_video_id=pos_action['video_id'],
                    count=int(ratio)
                )
                
                for video_id in negative_videos:
                    neg_action = {
                        **pos_action,
                        'video_id': video_id,
                        'action': 'skip',
                        'confidence': 0.8,  # æ¨å®šãªã®ã§ä¿¡é ¼åº¦ã¯ä½ã‚
                        'source': 'negative_sampling'
                    }
                    negative_actions.append(neg_action)
        
        return negative_actions
```

#### 2. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
```python
class TemporalDataGenerator:
    def generate_temporal_patterns(self, user_actions):
        """ãƒªã‚¢ãƒ«ãªæ™‚ç³»åˆ—ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        sessions = {}
        for user_id in set(action['user_id'] for action in user_actions):
            user_actions_sorted = sorted([a for a in user_actions if a['user_id'] == user_id], 
                                       key=lambda x: x['timestamp'])
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†å‰²ï¼ˆ1æ™‚é–“ä»¥ä¸Šç©ºã„ãŸã‚‰æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰
            sessions[user_id] = self.split_into_sessions(user_actions_sorted)
        
        return sessions
```

---

## ğŸ¤– MLå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### Two-Tower ãƒ¢ãƒ‡ãƒ«é©å¿œ

#### ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å­¦ç¿’ç”¨ã®ä¿®æ­£
```python
# ml_pipeline/models/two_tower_pseudo_user.py

class TwoTowerPseudoUserModel(TwoTowerModel):
    def __init__(self, config):
        super().__init__(config)
        
        # ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”¨ã®è¿½åŠ ãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.confidence_weight_layer = tf.keras.layers.Dense(1, activation='sigmoid')
        self.source_embedding = tf.keras.layers.Embedding(
            config.source_vocab_size, 32  # review_conversion, inferred_preference, negative_sampling
        )
    
    def call(self, inputs):
        user_features, item_features, confidence, source = inputs
        
        # åŸºæœ¬çš„ãªTwo-Towerå‡¦ç†
        user_repr = self.user_tower(user_features)
        item_repr = self.item_tower(item_features)
        
        # ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼åº¦ã‚’è€ƒæ…®
        confidence_weight = self.confidence_weight_layer(confidence)
        source_emb = self.source_embedding(source)
        
        # é¡ä¼¼åº¦è¨ˆç®—
        similarity = tf.keras.utils.cosine_similarity(user_repr, item_repr, axis=1)
        
        # ä¿¡é ¼åº¦ã§é‡ã¿ä»˜ã‘
        weighted_similarity = similarity * confidence_weight
        
        # æœ€çµ‚äºˆæ¸¬
        prediction = tf.nn.sigmoid(weighted_similarity + 
                                 tf.reduce_mean(source_emb, axis=1))
        
        return prediction
```

#### æ®µéšçš„å­¦ç¿’æˆ¦ç•¥
```python
class StageWiseLearning:
    def __init__(self, model):
        self.model = model
        self.learning_stages = [
            'pseudo_user_pretraining',
            'real_user_adaptation', 
            'continuous_learning'
        ]
    
    def stage1_pseudo_user_pretraining(self, pseudo_training_data):
        """æ®µéš1: ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã®äº‹å‰å­¦ç¿’"""
        
        print("Stage 1: Pseudo User Pre-training")
        
        # ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–ã—ãŸå­¦ç¿’è¨­å®š
        optimizer = tf.keras.optimizers.Adam(lr=0.001)
        loss_fn = self.create_confidence_weighted_loss()
        
        self.model.compile(optimizer=optimizer, loss=loss_fn, metrics=['auc'])
        
        # ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ã§ã®è¨“ç·´
        history = self.model.fit(
            pseudo_training_data,
            epochs=50,
            validation_split=0.2,
            callbacks=[
                EarlyStopping(patience=5),
                ReduceLROnPlateau(patience=3),
                ModelCheckpoint('pseudo_user_model.h5')
            ]
        )
        
        return history
    
    def stage2_real_user_adaptation(self, real_user_data, pseudo_model_weights):
        """æ®µéš2: å®Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã®é©å¿œå­¦ç¿’"""
        
        print("Stage 2: Real User Adaptation")
        
        # ç–‘ä¼¼å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.model.load_weights(pseudo_model_weights)
        
        # å®Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
        for layer in self.model.layers[:-3]:  # æœ€å¾Œã®3å±¤ä»¥å¤–ã¯å‡çµ
            layer.trainable = False
        
        # ã‚ˆã‚Šä½ã„å­¦ç¿’ç‡ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
        optimizer = tf.keras.optimizers.Adam(lr=0.0001)
        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['auc'])
        
        # å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’
        history = self.model.fit(
            real_user_data,
            epochs=20,
            validation_split=0.2
        )
        
        return history
    
    def create_confidence_weighted_loss(self):
        """ä¿¡é ¼åº¦ã‚’è€ƒæ…®ã—ãŸæå¤±é–¢æ•°"""
        def weighted_binary_crossentropy(y_true, y_pred, confidence):
            bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
            weighted_bce = bce * confidence
            return tf.reduce_mean(weighted_bce)
        
        return weighted_binary_crossentropy
```

---

## ğŸ” å“è³ªä¿è¨¼ãƒ»æ¤œè¨¼

### ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼

#### ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
```python
class PseudoUserDataValidator:
    def validate_generated_data(self, pseudo_users, user_actions):
        """ç”Ÿæˆã•ã‚ŒãŸç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼"""
        
        validation_results = {}
        
        # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
        validation_results['user_diversity'] = self.check_user_diversity(pseudo_users)
        
        # 2. è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¾å®Ÿæ€§ãƒã‚§ãƒƒã‚¯
        validation_results['behavior_realism'] = self.check_behavior_realism(user_actions)
        
        # 3. ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯  
        validation_results['data_balance'] = self.check_data_balance(user_actions)
        
        # 4. æ™‚ç³»åˆ—ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        validation_results['temporal_consistency'] = self.check_temporal_consistency(user_actions)
        
        return validation_results
    
    def check_user_diversity(self, pseudo_users):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¤šæ§˜æ€§æ¤œè¨¼"""
        user_profiles = [user['profile'] for user in pseudo_users.values()]
        
        return {
            'genre_preference_diversity': self.calculate_diversity(
                [profile.get('preferred_genres', {}) for profile in user_profiles]
            ),
            'rating_pattern_diversity': np.std([
                profile.get('avg_rating', 3.0) for profile in user_profiles
            ]),
            'activity_level_diversity': np.std([
                profile.get('review_count', 0) for profile in user_profiles  
            ])
        }
```

### ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¤œè¨¼

#### ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿å­¦ç¿’åŠ¹æœã®æ¸¬å®š
```python
class PseudoLearningEvaluator:
    def evaluate_pseudo_learning_effectiveness(self, model, test_data):
        """ç–‘ä¼¼å­¦ç¿’ã®åŠ¹æœã‚’æ¸¬å®š"""
        
        # 1. ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ
        random_performance = self.evaluate_random_baseline(test_data)
        model_performance = self.evaluate_model(model, test_data)
        
        improvement = {
            'auc_improvement': model_performance['auc'] - random_performance['auc'],
            'precision_improvement': model_performance['precision@10'] - random_performance['precision@10']
        }
        
        # 2. äººæ°—åº¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ  
        popularity_performance = self.evaluate_popularity_baseline(test_data)
        
        # 3. å¤šæ§˜æ€§ãƒ»æ–°è¦æ€§ã®æ¤œè¨¼
        diversity_metrics = self.calculate_diversity_metrics(model, test_data)
        
        return {
            'performance_improvements': improvement,
            'baseline_comparisons': {
                'vs_random': model_performance['auc'] / random_performance['auc'],
                'vs_popularity': model_performance['auc'] / popularity_performance['auc']
            },
            'diversity_metrics': diversity_metrics
        }
```

---

## ğŸ“‹ å®Ÿè£…è¨ˆç”»

### ã‚¿ã‚¹ã‚¯è©³ç´° (ãƒ•ã‚§ãƒ¼ã‚º1æ‹¡å¼µ)

#### T1.4: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿å¤§è¦æ¨¡åé›†ãƒ»å¤‰æ›
```
å„ªå…ˆåº¦: ğŸ”´ é«˜
å·¥æ•°è¦‹ç©: 24æ™‚é–“
æœŸé–“: Week 1-2
```

**ã‚µãƒ–ã‚¿ã‚¹ã‚¯:**
- [ ] **T1.4.1** - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ  _(8h)_
  - 10ä¸‡ä»¶ãƒ¬ãƒ“ãƒ¥ãƒ¼åé›†è‡ªå‹•åŒ–
  - è¤‡æ•°ã‚µã‚¤ãƒˆä¸¦è¡Œåé›†
  - å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»é‡è¤‡é™¤å»
- [ ] **T1.4.2** - ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ _(8h)_
  - ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼IDæŠ½å‡ºãƒ»åŒ¿ååŒ–
  - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ  
  - å—œå¥½ç‰¹å¾´é‡æŠ½å‡º
- [ ] **T1.4.3** - è¡Œå‹•ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚·ã‚¹ãƒ†ãƒ  _(8h)_
  - è©•ä¾¡å€¤â†’ã„ã„ã­/ã‚¹ã‚­ãƒƒãƒ—å¤‰æ›
  - é–¢é€£å‹•ç”»æ¨å®šè¡Œå‹•ç”Ÿæˆ
  - è² ä¾‹ãƒ»æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

#### T1.5: MLå­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ»æ¤œè¨¼
```  
å„ªå…ˆåº¦: ğŸ”´ é«˜
å·¥æ•°è¦‹ç©: 16æ™‚é–“
æœŸé–“: Week 2
```

**ã‚µãƒ–ã‚¿ã‚¹ã‚¯:**
- [ ] **T1.5.1** - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° _(6h)_
  - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡è¨­è¨ˆ
  - BERTåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- [ ] **T1.5.2** - ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  _(6h)_
  - ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
  - åˆ†å¸ƒãƒãƒ©ãƒ³ã‚¹æ¤œè¨¼
  - ç¾å®Ÿæ€§ã‚¹ã‚³ã‚¢ç®—å‡º
- [ ] **T1.5.3** - è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ _(4h)_
  - train/validation/teståˆ†å‰²
  - NPZå½¢å¼ã§ã®é«˜é€Ÿä¿å­˜
  - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±è¨ˆæƒ…å ±ç”Ÿæˆ

### æ›´æ–°ã•ã‚ŒãŸå®Œäº†æ¡ä»¶

**ãƒ•ã‚§ãƒ¼ã‚º1å®Œäº†æ¡ä»¶ (æ›´æ–°ç‰ˆ)**
- [ ] ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿10ä¸‡ä»¶ä»¥ä¸Šåé›†å®Œäº†
- [ ] **ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼5000äººä»¥ä¸Šç”Ÿæˆå®Œäº†**
- [ ] **ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿50ä¸‡ä»¶ä»¥ä¸Šç”Ÿæˆå®Œäº†**
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒå…¨ãƒ†ãƒ¼ãƒ–ãƒ«ç¨¼åƒ
- [ ] **ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢85%ä»¥ä¸Šé”æˆ**
- [ ] **MLå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†**

---

## ğŸ¯ æˆåŠŸæŒ‡æ¨™

### ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼å­¦ç¿’ã®æˆåŠŸæŒ‡æ¨™

#### ãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™
- [ ] **ãƒ¦ãƒ¼ã‚¶ãƒ¼å¤šæ§˜æ€§**: ã‚¸ãƒ£ãƒ³ãƒ«å—œå¥½ã®åˆ†æ•£ > 2.0
- [ ] **è¡Œå‹•ãƒªã‚¢ãƒªãƒ†ã‚£**: å®Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®é¡ä¼¼åº¦ > 0.7
- [ ] **ãƒ‡ãƒ¼ã‚¿ãƒãƒ©ãƒ³ã‚¹**: Like/Skipæ¯”ç‡ 40:60 ~ 60:40
- [ ] **æ™‚ç³»åˆ—ä¸€è²«æ€§**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¦¥å½“æ€§ > 0.8

#### å­¦ç¿’åŠ¹æœæŒ‡æ¨™  
- [ ] **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¶…è¶Š**: ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦ã«å¯¾ã—ã¦AUC +0.15ä»¥ä¸Š
- [ ] **å¤šæ§˜æ€§ä¿æŒ**: æ¨è–¦çµæœã®å¤šæ§˜æ€§æŒ‡æ•° > 0.6
- [ ] **å†·é–‹å§‹å¯¾å¿œ**: æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®æ¨è–¦ç²¾åº¦ > 0.65
- [ ] **å®Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼é©å¿œ**: å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æœ +0.05ä»¥ä¸Š

---

**æ–‡æ›¸ç®¡ç†**  
**ä½œæˆè€…**: Claude Code  
**æ‰¿èªè€…**: -  
**æ¬¡å›ãƒ¬ãƒ“ãƒ¥ãƒ¼äºˆå®š**: Week 2å®Œäº†æ™‚