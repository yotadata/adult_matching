name: Two-Tower Performance Benchmark

on:
  push:
    paths:
      - 'ml_pipeline/**'
      - 'tests/performance/**'
      - 'scripts/train_*.py'
  pull_request:
    paths:
      - 'ml_pipeline/**'
      - 'tests/performance/**'
      - 'scripts/train_*.py'
  schedule:
    # æ¯æ—¥åˆå‰9æ™‚ï¼ˆUTCï¼‰ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    - cron: '0 9 * * *'
  workflow_dispatch:

jobs:
  performance-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install UV
      uses: astral-sh/setup-uv@v3
      with:
        version: "0.8.11"

    - name: Install dependencies
      run: |
        uv sync

    - name: Create necessary directories
      run: |
        mkdir -p data_processing/processed_data
        mkdir -p ml_pipeline/models

    - name: Generate minimal test data (if needed)
      run: |
        # æœ€å°é™ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰
        if [ ! -f "data_processing/processed_data/rating_based_pseudo_users.json" ]; then
          echo "Generating minimal test data..."
          uv run python -c "
import json
import numpy as np
from pathlib import Path

# æœ€å°é™ã®ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿
pseudo_users = []
for i in range(10):
    user = {
        'id': i,
        'like_ratio': np.random.uniform(0.3, 0.9),
        'avg_rating': np.random.uniform(3.0, 5.0),
        'genre_preferences': {f'genre_{j}': np.random.uniform(0, 1) for j in range(3)}
    }
    pseudo_users.append(user)

# æœ€å°é™ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿
reviews = []
for i in range(100):
    review = {
        'content_id': f'content_{i}',
        'title': f'Test Content {i}',
        'rating': np.random.randint(1, 6),
        'genre': f'genre_{np.random.randint(0, 3)}',
        'maker': f'maker_{np.random.randint(0, 5)}'
    }
    reviews.append(review)

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆãƒ»ãƒ‡ãƒ¼ã‚¿ä¿å­˜
Path('data_processing/processed_data').mkdir(parents=True, exist_ok=True)
with open('data_processing/processed_data/rating_based_pseudo_users.json', 'w') as f:
    json.dump(pseudo_users, f)
with open('data_processing/processed_data/integrated_reviews.json', 'w') as f:
    json.dump(reviews, f)

print('Minimal test data generated successfully')
          "
        fi

    - name: Check if 768-dim model exists
      run: |
        if [ ! -d "ml_pipeline/models/rating_based_two_tower_768" ]; then
          echo "768-dim model not found, training minimal model for benchmarking..."
          # æœ€å°é™ã®768æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«è¨“ç·´
          uv run python scripts/train_768_dim_two_tower.py || echo "Model training failed, will use mock model in benchmark"
        else
          echo "768-dim model found, proceeding with benchmark"
        fi

    - name: Run Performance Benchmark
      run: |
        uv run python tests/performance/ci_benchmark.py
      env:
        CI: true

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-benchmark-results
        path: tests/performance/benchmark_results_*.json
        retention-days: 30

    - name: Generate performance report
      if: always()
      run: |
        echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
        
        # æœ€æ–°çµæœãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
        latest_result=$(ls -t tests/performance/benchmark_results_*.json 2>/dev/null | head -1 || echo "")
        
        if [ -n "$latest_result" ]; then
          echo "### ğŸ“Š Overall Results" >> $GITHUB_STEP_SUMMARY
          uv run python -c "
import json
import sys

try:
    with open('$latest_result') as f:
        results = json.load(f)
    
    summary = results.get('overall_summary', {})
    total_tests = summary.get('total_tests', 0)
    passed_tests = summary.get('passed_tests', 0)
    pass_rate = summary.get('pass_rate', 0) * 100
    
    status_emoji = 'âœ…' if summary.get('overall_passed', False) else 'âŒ'
    
    print(f'- {status_emoji} **Overall Status**: {\"PASSED\" if summary.get(\"overall_passed\", False) else \"FAILED\"}')
    print(f'- ğŸ“ˆ **Pass Rate**: {pass_rate:.1f}% ({passed_tests}/{total_tests})')
    print(f'- â±ï¸ **Total Execution Time**: {results.get(\"total_execution_time_minutes\", 0):.2f} minutes')
    
    print()
    print('### ğŸ“‹ Detailed Results')
    
    benchmarks = [
        ('æ¨è«–æ€§èƒ½', 'inference_benchmark'),
        ('è¨“ç·´æ€§èƒ½', 'training_benchmark'), 
        ('åŸ‹ã‚è¾¼ã¿æ›´æ–°', 'embedding_update_benchmark'),
        ('ãƒãƒƒãƒæ¨è–¦', 'batch_recommendation_benchmark')
    ]
    
    for name, key in benchmarks:
        if key in results:
            bench_result = results[key]
            if isinstance(bench_result, dict):
                passed = bench_result.get('overall_passed', False)
                status = 'âœ… PASS' if passed else 'âŒ FAIL'
                print(f'- **{name}**: {status}')
            else:
                print(f'- **{name}**: âš ï¸ Skipped/Error')
        else:
            print(f'- **{name}**: â“ No data')

except Exception as e:
    print(f'Error processing results: {e}', file=sys.stderr)
    sys.exit(1)
          " >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ No benchmark results found" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Comment on PR (if PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const glob = require('glob');
          
          // æœ€æ–°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
          const resultFiles = glob.sync('tests/performance/benchmark_results_*.json').sort().reverse();
          
          if (resultFiles.length === 0) {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'âŒ Performance benchmark results not found.'
            });
            return;
          }
          
          const results = JSON.parse(fs.readFileSync(resultFiles[0], 'utf8'));
          const summary = results.overall_summary || {};
          
          const overallPassed = summary.overall_passed || false;
          const passRate = (summary.pass_rate || 0) * 100;
          const totalTests = summary.total_tests || 0;
          const passedTests = summary.passed_tests || 0;
          const execTime = results.total_execution_time_minutes || 0;
          
          const statusIcon = overallPassed ? 'âœ…' : 'âŒ';
          const statusText = overallPassed ? 'PASSED' : 'FAILED';
          
          const comment = `## ${statusIcon} Performance Benchmark Results

**Overall Status**: ${statusText}
**Pass Rate**: ${passRate.toFixed(1)}% (${passedTests}/${totalTests})
**Execution Time**: ${execTime.toFixed(2)} minutes

### Individual Test Results:
- **æ¨è«–æ€§èƒ½**: ${results.inference_benchmark?.overall_passed ? 'âœ… PASS' : 'âŒ FAIL'}
- **è¨“ç·´æ€§èƒ½**: ${results.training_benchmark?.overall_passed ? 'âœ… PASS' : 'âŒ FAIL'} 
- **åŸ‹ã‚è¾¼ã¿æ›´æ–°**: ${results.embedding_update_benchmark?.overall_passed ? 'âœ… PASS' : 'âŒ FAIL'}
- **ãƒãƒƒãƒæ¨è–¦**: ${results.batch_recommendation_benchmark?.overall_passed ? 'âœ… PASS' : 'âŒ FAIL'}

${overallPassed ? 
  'ğŸ‰ All performance requirements are met!' : 
  'âš ï¸ Some performance requirements are not met. Please review the failing tests.'
}`;
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });