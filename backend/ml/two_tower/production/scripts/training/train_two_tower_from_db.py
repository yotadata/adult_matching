#!/usr/bin/env python3
"""
æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰Two-Towerãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

æœ¬ç•ªPostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€
768æ¬¡å…ƒTwo-Towerãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹
"""

import os
import sys
import json
import psycopg2
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import pickle

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæƒ…å ±
DB_URL = "postgresql://postgres.mfleexehdteobgsyokex:7Jh0iSwSQwXXtc62@aws-1-ap-northeast-1.pooler.supabase.com:5432/postgres"

# è¨­å®š
EMBEDDING_DIM = 768
BATCH_SIZE = 128
EPOCHS = 10
LEARNING_RATE = 0.001

def fetch_training_data():
    """æœ¬ç•ªDBã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    print("ğŸ“Š æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")

    conn = psycopg2.connect(DB_URL)

    # ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿å–å¾—
    videos_query = """
        SELECT
            v.id,
            v.title,
            v.description,
            v.maker,
            v.price,
            v.duration_seconds,
            COALESCE(array_agg(DISTINCT t.name) FILTER (WHERE t.name IS NOT NULL), ARRAY[]::text[]) as tags,
            COALESCE(array_agg(DISTINCT p.name) FILTER (WHERE p.name IS NOT NULL), ARRAY[]::text[]) as performers
        FROM videos v
        LEFT JOIN video_tags vt ON v.id = vt.video_id
        LEFT JOIN tags t ON vt.tag_id = t.id
        LEFT JOIN video_performers vp ON v.id = vp.video_id
        LEFT JOIN performers p ON vp.performer_id = p.id
        GROUP BY v.id, v.title, v.description, v.maker, v.price, v.duration_seconds
        LIMIT 10000
    """

    videos_df = pd.read_sql_query(videos_query, conn)
    print(f"âœ“ ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿å–å¾—: {len(videos_df)}ä»¶")

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—
    interactions_query = """
        SELECT
            uvd.user_id,
            uvd.video_id,
            CASE WHEN uvd.decision_type = 'like' THEN 1 ELSE 0 END as label,
            uvd.created_at
        FROM user_video_decisions uvd
        WHERE uvd.decision_type IN ('like', 'skip')
        ORDER BY uvd.created_at DESC
        LIMIT 50000
    """

    interactions_df = pd.read_sql_query(interactions_query, conn)
    print(f"âœ“ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(interactions_df)}ä»¶")

    # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã€ç–‘ä¼¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
    if len(interactions_df) < 1000:
        print(f"âš ï¸  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")

        # ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼IDç”Ÿæˆ
        pseudo_users = [f"pseudo_user_{i}" for i in range(100)]

        # å„ç–‘ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        pseudo_interactions = []
        for user_id in pseudo_users:
            # ãƒ©ãƒ³ãƒ€ãƒ ã«10-50æœ¬ã®ãƒ“ãƒ‡ã‚ªã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³
            num_interactions = np.random.randint(10, 51)
            sampled_videos = videos_df.sample(n=min(num_interactions, len(videos_df)))

            for _, video in sampled_videos.iterrows():
                # 80%ã®ç¢ºç‡ã§likeã€20%ã®ç¢ºç‡ã§skip
                decision = 1 if np.random.random() < 0.8 else 0
                pseudo_interactions.append({
                    'user_id': user_id,
                    'video_id': video['id'],
                    'label': decision,
                    'created_at': datetime.now()
                })

        pseudo_df = pd.DataFrame(pseudo_interactions)
        interactions_df = pd.concat([interactions_df, pseudo_df], ignore_index=True)
        print(f"âœ“ ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿è¿½åŠ å¾Œ: {len(interactions_df)}ä»¶ (å®Ÿãƒ‡ãƒ¼ã‚¿: {len(interactions_df) - len(pseudo_df)}ä»¶, ç–‘ä¼¼: {len(pseudo_df)}ä»¶)")

    conn.close()

    return videos_df, interactions_df

def prepare_features(videos_df, interactions_df):
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
    print("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸­...")

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    user_encoder = LabelEncoder()
    interactions_df['user_idx'] = user_encoder.fit_transform(interactions_df['user_id'])

    # ã‚¢ã‚¤ãƒ†ãƒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    item_encoder = LabelEncoder()
    videos_df['item_idx'] = item_encoder.fit_transform(videos_df['id'])

    # ã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡ä½œæˆ
    videos_df['title_len'] = videos_df['title'].str.len()
    videos_df['desc_len'] = videos_df['description'].fillna('').str.len()
    videos_df['price_normalized'] = videos_df['price'] / 10000
    videos_df['duration_normalized'] = videos_df['duration_seconds'] / 3600
    videos_df['num_tags'] = videos_df['tags'].apply(len)
    videos_df['num_performers'] = videos_df['performers'].apply(len)

    # ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    maker_encoder = LabelEncoder()
    videos_df['maker_idx'] = maker_encoder.fit_transform(videos_df['maker'].fillna('Unknown'))

    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
    merged_df = interactions_df.merge(
        videos_df[['id', 'item_idx', 'price_normalized', 'duration_normalized',
                   'num_tags', 'num_performers', 'maker_idx', 'title_len', 'desc_len']],
        left_on='video_id',
        right_on='id',
        how='inner'
    )

    print(f"âœ“ ãƒãƒ¼ã‚¸å¾Œãƒ‡ãƒ¼ã‚¿: {len(merged_df)}ä»¶")

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å¾´é‡ä½œæˆï¼ˆé›†ç´„çµ±è¨ˆï¼‰
    user_stats = merged_df.groupby('user_idx').agg({
        'label': 'sum',
        'price_normalized': 'mean',
        'duration_normalized': 'mean',
        'num_tags': 'mean',
        'num_performers': 'mean'
    }).reset_index()

    user_stats.columns = ['user_idx', 'total_likes', 'avg_price_pref',
                          'avg_duration_pref', 'avg_tags_pref', 'avg_performers_pref']

    # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    final_df = merged_df.merge(user_stats, on='user_idx', how='left')

    # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«åˆ†é›¢ï¼ˆfloat32ã«å¤‰æ›ï¼‰
    user_features = final_df[['user_idx', 'total_likes', 'avg_price_pref',
                               'avg_duration_pref', 'avg_tags_pref', 'avg_performers_pref']].values.astype(np.float32)

    item_features = final_df[['item_idx', 'price_normalized', 'duration_normalized',
                               'num_tags', 'num_performers', 'maker_idx',
                               'title_len', 'desc_len']].values.astype(np.float32)

    labels = final_df['label'].values.astype(np.float32)

    print(f"âœ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å¾´é‡: {user_features.shape}")
    print(f"âœ“ ã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡: {item_features.shape}")
    print(f"âœ“ ãƒ©ãƒ™ãƒ«: {labels.shape}")

    encoders = {
        'user_encoder': user_encoder,
        'item_encoder': item_encoder,
        'maker_encoder': maker_encoder,
        'num_users': len(user_encoder.classes_),
        'num_items': len(item_encoder.classes_),
        'num_makers': len(maker_encoder.classes_)
    }

    return user_features, item_features, labels, encoders

def build_two_tower_model(num_users, num_items, embedding_dim=768):
    """768æ¬¡å…ƒTwo-Towerãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
    print(f"ğŸ—ï¸  {embedding_dim}æ¬¡å…ƒTwo-Towerãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¸­...")

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¿ãƒ¯ãƒ¼
    user_input = layers.Input(shape=(6,), name='user_features')
    user_dense1 = layers.Dense(256, activation='relu')(user_input)
    user_bn1 = layers.BatchNormalization()(user_dense1)
    user_dropout1 = layers.Dropout(0.3)(user_bn1)
    user_dense2 = layers.Dense(512, activation='relu')(user_dropout1)
    user_bn2 = layers.BatchNormalization()(user_dense2)
    user_dropout2 = layers.Dropout(0.3)(user_bn2)
    user_embedding = layers.Dense(embedding_dim, activation='relu', name='user_embedding')(user_dropout2)

    # ã‚¢ã‚¤ãƒ†ãƒ ã‚¿ãƒ¯ãƒ¼
    item_input = layers.Input(shape=(8,), name='item_features')
    item_dense1 = layers.Dense(256, activation='relu')(item_input)
    item_bn1 = layers.BatchNormalization()(item_dense1)
    item_dropout1 = layers.Dropout(0.3)(item_bn1)
    item_dense2 = layers.Dense(512, activation='relu')(item_dropout1)
    item_bn2 = layers.BatchNormalization()(item_dense2)
    item_dropout2 = layers.Dropout(0.3)(item_bn2)
    item_embedding = layers.Dense(embedding_dim, activation='relu', name='item_embedding')(item_dropout2)

    # ãƒ‰ãƒƒãƒˆç©
    dot_product = layers.Dot(axes=1, normalize=True)([user_embedding, item_embedding])

    # å‡ºåŠ›å±¤
    output = layers.Dense(1, activation='sigmoid')(dot_product)

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = models.Model(inputs=[user_input, item_input], outputs=output)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¿ãƒ¯ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã‚¿ãƒ¯ãƒ¼ã‚’å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æŠ½å‡º
    user_tower = models.Model(inputs=user_input, outputs=user_embedding, name='user_tower')
    item_tower = models.Model(inputs=item_input, outputs=item_embedding, name='item_tower')

    print(f"âœ“ Full Model: {model.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print(f"âœ“ User Tower: {user_tower.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print(f"âœ“ Item Tower: {item_tower.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    return model, user_tower, item_tower

def train_model(model, user_features, item_features, labels):
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    print("ğŸš€ ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_user_train, X_user_val, X_item_train, X_item_val, y_train, y_val = train_test_split(
        user_features, item_features, labels, test_size=0.2, random_state=42
    )

    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    model.compile(
        optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    )

    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)
    ]

    # è¨“ç·´
    history = model.fit(
        [X_user_train, X_item_train],
        y_train,
        validation_data=([X_user_val, X_item_val], y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=callbacks,
        verbose=1
    )

    # è©•ä¾¡
    val_loss, val_acc, val_auc = model.evaluate([X_user_val, X_item_val], y_val, verbose=0)
    print(f"\nâœ“ æ¤œè¨¼æå¤±: {val_loss:.4f}")
    print(f"âœ“ æ¤œè¨¼ç²¾åº¦: {val_acc:.4f}")
    print(f"âœ“ æ¤œè¨¼AUC: {val_auc:.4f}")

    return history

def save_models(model, user_tower, item_tower, encoders):
    """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")

    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    save_dir = Path("/home/devel/dev/adult_matching/backend/ml/models/two_tower_768_production")
    save_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save(save_dir / "full_model_768.keras")
    user_tower.save(save_dir / "user_tower_768.keras")
    item_tower.save(save_dir / "item_tower_768.keras")

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¿å­˜
    with open(save_dir / "encoders_768.pkl", "wb") as f:
        pickle.dump(encoders, f)

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        "model_type": "two_tower_768",
        "embedding_dim": EMBEDDING_DIM,
        "trained_at": datetime.now().isoformat(),
        "num_users": encoders['num_users'],
        "num_items": encoders['num_items'],
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "learning_rate": LEARNING_RATE
    }

    with open(save_dir / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)

    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {save_dir}")

def main():
    print("=" * 60)
    print("ğŸ¤– 768æ¬¡å…ƒTwo-Towerãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆæœ¬ç•ªDBä½¿ç”¨ï¼‰")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    videos_df, interactions_df = fetch_training_data()

    # ç‰¹å¾´é‡æº–å‚™
    user_features, item_features, labels, encoders = prepare_features(videos_df, interactions_df)

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    model, user_tower, item_tower = build_two_tower_model(
        encoders['num_users'],
        encoders['num_items'],
        EMBEDDING_DIM
    )

    # è¨“ç·´
    history = train_model(model, user_features, item_features, labels)

    # ä¿å­˜
    save_models(model, user_tower, item_tower, encoders)

    print("\nğŸ‰ è¨“ç·´å®Œäº†ï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()