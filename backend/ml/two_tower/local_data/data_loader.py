#!/usr/bin/env python3
"""
Enhanced Data Loader for Two-Tower Model
è±Šå¯Œãªç‰¹å¾´é‡ã‚’æ´»ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
- ã‚¿ã‚°æƒ…å ±ã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®æ´»ç”¨
- ãƒ¡ãƒ¼ã‚«ãƒ¼ãƒ»ã‚·ãƒªãƒ¼ã‚ºãƒ»ç›£ç£ã®åŸ‹ã‚è¾¼ã¿
- å…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime
from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer
from collections import Counter
import re

class SupabaseDataLoader:
    """è±Šå¯Œãªç‰¹å¾´é‡ã‚’æ´»ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"""

    def __init__(self):
        # Supabaseå•†ç”¨DBäº’æ›ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        self.data_dir = Path("../../../data_processing/local_compatible_data")

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä¿å­˜
        self.encoders = {}
        self.scalers = {}

        # ãƒ‡ãƒ¼ã‚¿
        self.profiles_df = None
        self.videos_df = None
        self.decisions_df = None

        # ç‰¹å¾´é‡é–¢é€£
        self.tag_binarizer = MultiLabelBinarizer()
        self.all_tags = set()
        self.frequent_tags = set()  # é »å‡ºã‚¿ã‚°ã‚»ãƒƒãƒˆ
        self.text_features = {}

    def load_all_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ“Š Enhanced Data Loading...")

        self.profiles_df = self._load_profiles()
        self.videos_df = self._load_videos()
        self.decisions_df = self._load_decisions()

        print(f"âœ… Loaded: {len(self.profiles_df)} users, {len(self.videos_df)} videos, {len(self.decisions_df)} decisions")

        # ã‚¿ã‚°åˆ†æ
        self._analyze_tags()

        # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡åˆ†æ
        self._analyze_text_features()

    def _load_profiles(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        with open(self.data_dir / "profiles.json", 'r', encoding='utf-8') as f:
            profiles = json.load(f)

        # æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’ãƒ‘ãƒ¼ã‚¹
        for profile in profiles:
            if profile.get('created_at'):
                profile['created_at'] = pd.to_datetime(profile['created_at'])

        return pd.DataFrame(profiles)

    def _load_videos(self):
        """å‹•ç”»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        with open(self.data_dir / "videos_subset.json", 'r', encoding='utf-8') as f:
            videos = json.load(f)

        # æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’ãƒ‘ãƒ¼ã‚¹
        for video in videos:
            if video.get('product_released_at'):
                video['product_released_at'] = pd.to_datetime(video['product_released_at'])
            if video.get('created_at'):
                video['created_at'] = pd.to_datetime(video['created_at'])

        return pd.DataFrame(videos)

    def _load_decisions(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å‹•ç”»æ±ºå®šãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        with open(self.data_dir / "user_video_decisions.json", 'r', encoding='utf-8') as f:
            decisions = json.load(f)

        # æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’ãƒ‘ãƒ¼ã‚¹
        for decision in decisions:
            if decision.get('created_at'):
                decision['created_at'] = pd.to_datetime(decision['created_at'])

        return pd.DataFrame(decisions)

    def _analyze_tags(self):
        """ã‚¿ã‚°åˆ†æã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æº–å‚™"""
        print("ğŸ·ï¸ Analyzing tags...")

        # å…¨ã‚¿ã‚°ã‚’åé›†
        all_tags = []
        for _, video in self.videos_df.iterrows():
            if isinstance(video['tags'], list):
                all_tags.extend(video['tags'])
                self.all_tags.update(video['tags'])

        # ã‚¿ã‚°é »åº¦åˆ†æ
        tag_counts = Counter(all_tags)
        print(f"ğŸ“ˆ Total unique tags: {len(tag_counts)}")
        print(f"ğŸ“ˆ Top 10 tags: {tag_counts.most_common(10)}")

        # é »åº¦ã®ä½ã„ã‚¿ã‚°ã‚’é™¤å¤–ï¼ˆ3å›æœªæº€ï¼‰
        frequent_tags = {tag for tag, count in tag_counts.items() if count >= 3}
        self.frequent_tags = frequent_tags
        print(f"ğŸ“ˆ Frequent tags (â‰¥3 occurrences): {len(frequent_tags)}")

        # MultiLabelBinarizerã‚’æº–å‚™
        video_tags = []
        for _, video in self.videos_df.iterrows():
            if isinstance(video['tags'], list):
                # é »åº¦ã®é«˜ã„ã‚¿ã‚°ã®ã¿ä½¿ç”¨
                filtered_tags = [tag for tag in video['tags'] if tag in frequent_tags]
                video_tags.append(filtered_tags)
            else:
                video_tags.append([])

        self.tag_binarizer.fit(video_tags)
        print(f"ğŸ“ˆ Tag binarizer vocabulary: {len(self.tag_binarizer.classes_)}")

    def _analyze_text_features(self):
        """ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡åˆ†æ"""
        print("ğŸ“ Analyzing text features...")

        # ã‚¿ã‚¤ãƒˆãƒ«é•·ã®çµ±è¨ˆ
        title_lengths = self.videos_df['title'].str.len()
        print(f"ğŸ“ˆ Title length: mean={title_lengths.mean():.1f}, max={title_lengths.max()}")

        # ãƒ¡ãƒ¼ã‚«ãƒ¼ã€ç›£ç£ã€ã‚·ãƒªãƒ¼ã‚ºã®çµ±è¨ˆ
        for col in ['maker', 'director', 'series']:
            if col in self.videos_df.columns:
                unique_count = self.videos_df[col].nunique()
                print(f"ğŸ“ˆ {col}: {unique_count} unique values")

    def get_realtime_training_data(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‹å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆLIKE/NOPEå±¥æ­´ãƒ™ãƒ¼ã‚¹ï¼‰"""
        print("ğŸ”„ Creating realtime training data...")

        # å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®LIKE/NOPEå±¥æ­´ã‚’å–å¾—
        training_samples = []

        for user_id in self.decisions_df['user_id'].unique():
            user_decisions = self.decisions_df[self.decisions_df['user_id'] == user_id]

            # LIKEå±¥æ­´ã¨NOPEå±¥æ­´ã‚’åˆ†åˆ¥
            liked_videos = user_decisions[user_decisions['decision_type'] == 'like']['video_id'].tolist()
            noped_videos = user_decisions[user_decisions['decision_type'] == 'nope']['video_id'].tolist()

            # å„æ±ºå®šã«å¯¾ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ä½œæˆ
            for _, decision in user_decisions.iterrows():
                # ãã®æ±ºå®šæ™‚ç‚¹ã§ã®å±¥æ­´ã‚’ä½œæˆï¼ˆæ™‚ç³»åˆ—è€ƒæ…®ï¼‰
                decision_time = decision['created_at']

                # ãã®æ™‚ç‚¹ã‚ˆã‚Šå‰ã®LIKE/NOPEå±¥æ­´
                prior_decisions = user_decisions[user_decisions['created_at'] < decision_time]
                prior_likes = prior_decisions[prior_decisions['decision_type'] == 'like']['video_id'].tolist()
                prior_nopes = prior_decisions[prior_decisions['decision_type'] == 'nope']['video_id'].tolist()

                # å‹•ç”»æƒ…å ±ã‚’å–å¾—
                video_info = self.videos_df[self.videos_df['id'] == decision['video_id']].iloc[0]

                sample = {
                    'user_id': user_id,
                    'video_id': decision['video_id'],
                    'user_like_history': prior_likes,
                    'user_nope_history': prior_nopes,
                    'video_features': video_info,
                    'label': 1 if decision['decision_type'] == 'like' else 0
                }
                training_samples.append(sample)

        print(f"ğŸ“ˆ Realtime training data: {len(training_samples)} samples")
        return training_samples

    def get_enhanced_user_features(self, training_data: pd.DataFrame) -> np.ndarray:
        """æ‹¡å¼µãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
        print("ğŸ‘¤ Creating enhanced user features...")

        user_features = []

        for user_id in training_data['user_id'].unique():
            user_data = training_data[training_data['user_id'] == user_id].iloc[0]
            user_decisions = training_data[training_data['user_id'] == user_id]

            # åŸºæœ¬ç‰¹å¾´é‡
            basic_features = [
                len(user_data['display_name']) if pd.notna(user_data['display_name']) else 0,  # display_name_length
                (datetime.now() - user_data['created_at_user']).days,  # account_age_days
                len(user_decisions),  # total_decisions
                user_decisions['decision_type'].apply(lambda x: 1 if x == 'like' else 0).mean()  # like_ratio
            ]

            # è¡Œå‹•ç‰¹å¾´é‡ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’åæ˜ ï¼‰
            liked_videos = user_decisions[user_decisions['decision_type'] == 'like']

            # ä¾¡æ ¼å¸¯ã®å¥½ã¿
            if len(liked_videos) > 0:
                avg_liked_price = liked_videos['price'].mean() if 'price' in liked_videos.columns else 0
                price_preference_high = (liked_videos['price'] > 1000).mean() if 'price' in liked_videos.columns else 0
            else:
                avg_liked_price = 0
                price_preference_high = 0

            behavioral_features = [
                avg_liked_price,  # å¹³å‡ä¾¡æ ¼å¸¯
                price_preference_high,  # é«˜ä¾¡æ ¼å¸¯å¥½ã¿ç‡
                len(liked_videos),  # likeæ•°
                len(user_decisions) - len(liked_videos)  # nopeæ•°
            ]

            # ã‚¿ã‚°ã®å¥½ã¿åˆ†æ
            if len(liked_videos) > 0:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒlikeã—ãŸå‹•ç”»ã®ã‚¿ã‚°ã‚’é›†è¨ˆ
                liked_tags = []
                for _, video in liked_videos.iterrows():
                    if isinstance(video['tags'], list):
                        liked_tags.extend([tag for tag in video['tags'] if tag in self.frequent_tags])

                tag_preferences = Counter(liked_tags)
                # ä¸Šä½5ã¤ã®ã‚¿ã‚°å¥½ã¿ã‚’ç‰¹å¾´é‡ã«
                top_tag_prefs = [tag_preferences.get(tag, 0) for tag in list(self.frequent_tags)[:5]]
            else:
                top_tag_prefs = [0] * 5

            # å…¨ç‰¹å¾´é‡çµåˆ
            features = basic_features + behavioral_features + top_tag_prefs
            user_features.append(features)

        user_features_array = np.array(user_features)
        print(f"ğŸ‘¤ User features shape: {user_features_array.shape}")

        return user_features_array

    def get_enhanced_video_features(self, training_data: pd.DataFrame) -> np.ndarray:
        """æ‹¡å¼µå‹•ç”»ç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
        print("ğŸ¬ Creating enhanced video features...")

        video_features = []

        for video_id in training_data['video_id'].unique():
            video_data = training_data[training_data['video_id'] == video_id].iloc[0]
            video_decisions = training_data[training_data['video_id'] == video_id]

            # åŸºæœ¬ç‰¹å¾´é‡
            basic_features = [
                len(video_data['title']) if pd.notna(video_data['title']) else 0,  # title_length
                video_data['price'] if pd.notna(video_data['price']) else 0,  # price
                video_data['product_released_at'].year if pd.notna(video_data['product_released_at']) else 2024,  # release_year
                1 if pd.notna(video_data['thumbnail_url']) else 0,  # has_thumbnail
                len(video_decisions),  # total_decisions
                video_decisions['decision_type'].apply(lambda x: 1 if x == 'like' else 0).mean()  # like_ratio
            ]

            # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
            metadata_features = [
                len(video_data['maker']) if pd.notna(video_data['maker']) else 0,  # maker_length
                len(video_data['director']) if pd.notna(video_data['director']) else 0,  # director_length
                len(video_data['series']) if pd.notna(video_data['series']) else 0,  # series_length
                1 if pd.notna(video_data['sample_video_url']) else 0,  # has_sample
                1 if pd.notna(video_data['preview_video_url']) else 0,  # has_preview
                len(video_data['image_urls']) if isinstance(video_data['image_urls'], list) else 0,  # image_count
                video_data['duration_seconds'] if pd.notna(video_data['duration_seconds']) else 0  # duration
            ]

            # ã‚¿ã‚°ç‰¹å¾´é‡ï¼ˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆï¼‰
            if isinstance(video_data['tags'], list):
                filtered_tags = [tag for tag in video_data['tags'] if tag in self.frequent_tags]
                tag_features = self.tag_binarizer.transform([filtered_tags])[0]
            else:
                tag_features = self.tag_binarizer.transform([[]])[0]

            # å…¨ç‰¹å¾´é‡çµåˆ
            features = basic_features + metadata_features + tag_features.tolist()
            video_features.append(features)

        video_features_array = np.array(video_features)
        print(f"ğŸ¬ Video features shape: {video_features_array.shape}")

        return video_features_array

    def get_single_user_features(self, row: pd.Series) -> np.ndarray:
        """å˜ä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‹¡å¼µç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
        user_data = row

        # åŸºæœ¬ç‰¹å¾´é‡
        basic_features = [
            len(user_data['display_name']) if pd.notna(user_data['display_name']) else 0,  # display_name_length
            (datetime.now() - user_data['created_at_user']).days,  # account_age_days
            1,  # ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã®æ±ºå®šæ•°
            1 if user_data['decision_type'] == 'like' else 0  # ã“ã®æ±ºå®šã®like/nope
        ]

        # è¡Œå‹•ç‰¹å¾´é‡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        behavioral_features = [
            user_data['price'] if pd.notna(user_data['price']) else 0,  # ã“ã®å‹•ç”»ã®ä¾¡æ ¼ï¼ˆå¥½ã¿æ¨å®šç”¨ï¼‰
            1 if user_data['price'] > 1000 else 0,  # é«˜ä¾¡æ ¼å¸¯åˆ¤å®š
            1 if user_data['decision_type'] == 'like' else 0,  # likeåˆ¤å®š
            1 if user_data['decision_type'] == 'nope' else 0   # nopeåˆ¤å®š
        ]

        # ã‚¿ã‚°ã®å¥½ã¿åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if isinstance(user_data['tags'], list) and user_data['decision_type'] == 'like':
            liked_tags = [tag for tag in user_data['tags'] if tag in self.frequent_tags]
            top_tag_prefs = [1 if tag in liked_tags else 0 for tag in list(self.frequent_tags)[:5]]
        else:
            top_tag_prefs = [0] * 5

        # å…¨ç‰¹å¾´é‡çµåˆ
        features = basic_features + behavioral_features + top_tag_prefs
        return np.array(features)

    def get_single_video_features(self, row: pd.Series) -> np.ndarray:
        """å˜ä¸€å‹•ç”»ã®æ‹¡å¼µç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
        video_data = row

        # åŸºæœ¬ç‰¹å¾´é‡
        basic_features = [
            len(video_data['title']) if pd.notna(video_data['title']) else 0,  # title_length
            video_data['price'] if pd.notna(video_data['price']) else 0,  # price
            video_data['product_released_at'].year if pd.notna(video_data['product_released_at']) else 2024,  # release_year
            1 if pd.notna(video_data['thumbnail_url']) else 0,  # has_thumbnail
            1,  # ã“ã®å‹•ç”»ã§ã®æ±ºå®šæ•°ï¼ˆ1ã¤ï¼‰
            1 if video_data['decision_type'] == 'like' else 0  # likeç‡ï¼ˆ0ã¾ãŸã¯1ï¼‰
        ]

        # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
        metadata_features = [
            len(video_data['maker']) if pd.notna(video_data['maker']) else 0,  # maker_length
            len(video_data['director']) if pd.notna(video_data['director']) else 0,  # director_length
            len(video_data['series']) if pd.notna(video_data['series']) else 0,  # series_length
            1 if pd.notna(video_data['sample_video_url']) else 0,  # has_sample
            1 if pd.notna(video_data['preview_video_url']) else 0,  # has_preview
            len(video_data['image_urls']) if isinstance(video_data['image_urls'], list) else 0,  # image_count
            video_data['duration_seconds'] if pd.notna(video_data['duration_seconds']) else 0  # duration
        ]

        # ã‚¿ã‚°ç‰¹å¾´é‡ï¼ˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆï¼‰
        if isinstance(video_data['tags'], list):
            filtered_tags = [tag for tag in video_data['tags'] if tag in self.frequent_tags]
            tag_features = self.tag_binarizer.transform([filtered_tags])[0]
        else:
            tag_features = self.tag_binarizer.transform([[]])[0]

        # å…¨ç‰¹å¾´é‡çµåˆ
        features = basic_features + metadata_features + tag_features.tolist()
        return np.array(features)

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    loader = SupabaseDataLoader()
    loader.load_all_data()

    training_data, labels = loader.get_enhanced_training_data()
    user_features = loader.get_enhanced_user_features(training_data)
    video_features = loader.get_enhanced_video_features(training_data)

    print(f"\nğŸ¯ Final Summary:")
    print(f"   Training samples: {len(labels)}")
    print(f"   User features: {user_features.shape[1]} dimensions")
    print(f"   Video features: {video_features.shape[1]} dimensions")
    print(f"   Tag vocabulary: {len(loader.tag_binarizer.classes_)} tags")