"""
ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æ
"""

import asyncio
import aiohttp
import json
import os
from pathlib import Path
from collections import Counter

async def comprehensive_analysis():
    supabase_url = os.getenv('NEXT_PUBLIC_SUPABASE_URL')
    supabase_key = os.getenv('NEXT_PUBLIC_SUPABASE_ANON_KEY')

    print("=== åŒ…æ‹¬çš„ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æ ===")

    # 1. ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å…¨content_idå–å¾—
    processed_data_dir = Path("../processed_data")
    reviews_file = processed_data_dir / "integrated_reviews.json"

    with open(reviews_file, 'r', encoding='utf-8') as f:
        reviews = json.load(f)

    review_content_ids = set()
    for review in reviews:
        content_id = review.get('content_id')
        if content_id:
            review_content_ids.add(content_id)

    print(f"1. ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿")
    print(f"   ç·ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°: {len(reviews):,}")
    print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯content_idæ•°: {len(review_content_ids):,}")

    # content_idã®å½¢å¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    patterns = Counter()
    for content_id in list(review_content_ids)[:1000]:  # ã‚µãƒ³ãƒ—ãƒ«
        if content_id:
            # æ–‡å­—æ•°ã«ã‚ˆã‚‹åˆ†é¡
            length = len(content_id)
            patterns[f"æ–‡å­—æ•°{length}"] += 1

            # æ•°å­—ã®æœ‰ç„¡
            has_numbers = any(c.isdigit() for c in content_id)
            patterns[f"æ•°å­—{'æœ‰' if has_numbers else 'ç„¡'}"] += 1

    print(f"   content_idå½¢å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ (ã‚µãƒ³ãƒ—ãƒ«1000ä»¶):")
    for pattern, count in patterns.most_common(10):
        print(f"     {pattern}: {count}")

    print(f"   content_idã‚µãƒ³ãƒ—ãƒ«: {list(review_content_ids)[:15]}")

    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å…¨å‹•ç”»ãƒ‡ãƒ¼ã‚¿å–å¾—
    async with aiohttp.ClientSession() as session:
        headers = {
            'apikey': supabase_key,
            'Authorization': f'Bearer {supabase_key}'
        }

        # å…¨å‹•ç”»æ•°ã‚’å–å¾—
        count_url = f"{supabase_url}/rest/v1/videos"
        count_params = {'select': 'count'}

        try:
            async with session.head(count_url, headers=headers) as response:
                total_videos = int(response.headers.get('Content-Range', '0').split('/')[-1])

        except:
            total_videos = 0

        print(f"\\n2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‹•ç”»ãƒ‡ãƒ¼ã‚¿")
        print(f"   ç·å‹•ç”»æ•°: {total_videos:,}")

        # å…¨external_idã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        db_external_ids = set()
        limit = 1000
        offset = 0

        while True:
            videos_url = f"{supabase_url}/rest/v1/videos"
            params = {
                'select': 'external_id',
                'limit': limit,
                'offset': offset
            }

            try:
                async with session.get(videos_url, headers=headers, params=params) as response:
                    if response.status == 200:
                        videos = await response.json()
                        if not videos:
                            break

                        for video in videos:
                            external_id = video.get('external_id')
                            if external_id:
                                db_external_ids.add(external_id)

                        offset += limit

                        if len(videos) < limit:
                            break

                    else:
                        print(f"APIå–å¾—å¤±æ•—: {response.status}")
                        break

            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼: {e}")
                break

        print(f"   external_idæœ‰ã‚Šå‹•ç”»æ•°: {len(db_external_ids):,}")

        # external_idã®å½¢å¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        db_patterns = Counter()
        for external_id in list(db_external_ids)[:1000]:  # ã‚µãƒ³ãƒ—ãƒ«
            if external_id:
                length = len(external_id)
                db_patterns[f"æ–‡å­—æ•°{length}"] += 1

                has_numbers = any(c.isdigit() for c in external_id)
                db_patterns[f"æ•°å­—{'æœ‰' if has_numbers else 'ç„¡'}"] += 1

        print(f"   external_idå½¢å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ (ã‚µãƒ³ãƒ—ãƒ«1000ä»¶):")
        for pattern, count in db_patterns.most_common(10):
            print(f"     {pattern}: {count}")

        print(f"   external_idã‚µãƒ³ãƒ—ãƒ«: {list(db_external_ids)[:15]}")

        # 3. ãƒãƒƒãƒãƒ³ã‚°åˆ†æ
        print(f"\\n3. ãƒãƒƒãƒãƒ³ã‚°åˆ†æ")
        exact_matches = review_content_ids.intersection(db_external_ids)
        print(f"   å®Œå…¨ä¸€è‡´: {len(exact_matches):,}ä»¶")

        if exact_matches:
            print(f"   ä¸€è‡´ä¾‹: {list(exact_matches)[:10]}")

        # éƒ¨åˆ†ä¸€è‡´åˆ†æï¼ˆå…ˆé ­éƒ¨åˆ†ï¼‰
        partial_matches = 0
        for review_id in list(review_content_ids)[:100]:  # ã‚µãƒ³ãƒ—ãƒ«
            for db_id in list(db_external_ids)[:100]:  # ã‚µãƒ³ãƒ—ãƒ«
                if review_id in db_id or db_id in review_id:
                    partial_matches += 1
                    break

        print(f"   éƒ¨åˆ†ä¸€è‡´ï¼ˆã‚µãƒ³ãƒ—ãƒ«100x100ï¼‰: {partial_matches}ä»¶")

        # 4. çµ±è¨ˆã‚µãƒãƒªãƒ¼
        print(f"\\n4. çµ±è¨ˆã‚µãƒãƒªãƒ¼")
        review_coverage = len(exact_matches) / len(review_content_ids) * 100 if review_content_ids else 0
        db_coverage = len(exact_matches) / len(db_external_ids) * 100 if db_external_ids else 0

        print(f"   ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¼ç‡: {review_coverage:.2f}%")
        print(f"   DBãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¼ç‡: {db_coverage:.2f}%")

        # 5. æ¨å¥¨å¯¾å¿œç­–
        print(f"\\n5. æ¨å¥¨å¯¾å¿œç­–")
        if len(exact_matches) == 0:
            print("   âŒ å®Œå…¨ä¸€è‡´ãªã— - ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãŒç•°ãªã‚‹å¯èƒ½æ€§")
            print("   ğŸ“‹ å¯¾ç­–æ¡ˆ:")
            print("     1. content_idã®å½¢å¼å¤‰æ›ãƒ«ãƒ¼ãƒ«èª¿æŸ»")
            print("     2. åˆ¥ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚­ãƒ¼ä½¿ç”¨ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç­‰ï¼‰")
            print("     3. DMM APIã§ã®å‹•ç”»ãƒ‡ãƒ¼ã‚¿å†åŒæœŸ")
        elif len(exact_matches) < len(review_content_ids) * 0.5:
            print("   âš ï¸  ä¸€è‡´ç‡ä½ã„ - éƒ¨åˆ†çš„ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        else:
            print("   âœ… ä¸€è‡´ç‡è‰¯å¥½ - å‡¦ç†å¯èƒ½")

if __name__ == "__main__":
    asyncio.run(comprehensive_analysis())