"""
Test Runner

åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import sys
import subprocess
import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import pytest


class TestCategory(Enum):
    """ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒª"""
    UNIT = "unit"
    INTEGRATION = "integration"
    E2E = "e2e"
    PERFORMANCE = "performance"
    ML = "ml"
    ALL = "all"


class TestResult(Enum):
    """ãƒ†ã‚¹ãƒˆçµæœ"""
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"


@dataclass
class TestRun:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæƒ…å ±"""
    category: TestCategory
    start_time: float
    end_time: Optional[float] = None
    result: Optional[TestResult] = None
    tests_passed: int = 0
    tests_failed: int = 0
    tests_skipped: int = 0
    tests_total: int = 0
    duration: float = 0.0
    coverage: Optional[float] = None
    exit_code: int = 0
    output: str = ""
    error: str = ""


class TestRunner:
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼"""
    
    def __init__(self, base_path: Optional[Path] = None):
        self.base_path = base_path or Path(__file__).parent
        self.project_root = self.base_path.parent
        self.test_results: List[TestRun] = []
        
        # Test directories
        self.test_dirs = {
            TestCategory.UNIT: self.base_path / "unit",
            TestCategory.INTEGRATION: self.base_path / "integration", 
            TestCategory.E2E: self.base_path / "e2e",
            TestCategory.PERFORMANCE: self.base_path / "performance",
            TestCategory.ML: self.base_path / "ml"
        }
    
    def run_tests(self, categories: List[TestCategory], 
                  coverage: bool = True,
                  parallel: bool = False,
                  verbose: bool = False,
                  fail_fast: bool = False) -> List[TestRun]:
        """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("ğŸš€ åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹")
        print("=" * 60)
        
        results = []
        
        for category in categories:
            if category == TestCategory.ALL:
                # Run all categories
                all_categories = [c for c in TestCategory if c != TestCategory.ALL]
                for cat in all_categories:
                    result = self._run_category(cat, coverage, parallel, verbose, fail_fast)
                    results.append(result)
                    if fail_fast and result.result == TestResult.FAILED:
                        break
            else:
                result = self._run_category(category, coverage, parallel, verbose, fail_fast)
                results.append(result)
                if fail_fast and result.result == TestResult.FAILED:
                    break
        
        self.test_results.extend(results)
        self._print_summary(results)
        return results
    
    def _run_category(self, category: TestCategory, 
                     coverage: bool = True,
                     parallel: bool = False,
                     verbose: bool = False,
                     fail_fast: bool = False) -> TestRun:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        test_run = TestRun(category=category, start_time=time.time())
        
        test_dir = self.test_dirs.get(category)
        if not test_dir or not test_dir.exists():
            print(f"âš ï¸  ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_dir}")
            test_run.end_time = time.time()
            test_run.result = TestResult.SKIPPED
            test_run.duration = test_run.end_time - test_run.start_time
            return test_run
        
        print(f"\nğŸ“ {category.value.upper()} ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        # Build pytest command
        cmd = ["python", "-m", "pytest", str(test_dir)]
        
        # Add options
        if verbose:
            cmd.append("-v")
        
        if fail_fast:
            cmd.append("-x")
        
        if parallel:
            cmd.extend(["-n", "auto"])
        
        if coverage:
            cmd.extend([
                "--cov=backend",
                f"--cov-report=html:htmlcov_{category.value}",
                "--cov-report=term-missing"
            ])
        
        # Add markers based on category
        if category == TestCategory.UNIT:
            cmd.extend(["-m", "unit"])
        elif category == TestCategory.INTEGRATION:
            cmd.extend(["-m", "integration"])
        elif category == TestCategory.PERFORMANCE:
            cmd.extend(["-m", "performance"])
        elif category == TestCategory.ML:
            cmd.extend(["-m", "ml"])
        
        # Add JSON report
        json_report = self.base_path / f"report_{category.value}.json"
        cmd.extend(["--json-report", f"--json-report-file={json_report}"])
        
        try:
            # Set environment variables
            env = os.environ.copy()
            env["PYTHONPATH"] = str(self.project_root)
            
            # Special environment for different test categories
            if category == TestCategory.INTEGRATION:
                env["RUN_INTEGRATION_TESTS"] = "1"
            elif category == TestCategory.PERFORMANCE:
                env["RUN_PERFORMANCE_TESTS"] = "1"
            
            # Run tests
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                env=env,
                capture_output=True,
                text=True,
                timeout=1800  # 30 minutes timeout
            )
            
            test_run.exit_code = result.returncode
            test_run.output = result.stdout
            test_run.error = result.stderr
            
            # Parse JSON report if available
            if json_report.exists():
                self._parse_json_report(test_run, json_report)
            
            # Determine result
            if result.returncode == 0:
                test_run.result = TestResult.PASSED
                print(f"âœ… {category.value.upper()} ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            else:
                test_run.result = TestResult.FAILED
                print(f"âŒ {category.value.upper()} ãƒ†ã‚¹ãƒˆå¤±æ•—")
                if verbose:
                    print(f"Error output:\n{result.stderr}")
        
        except subprocess.TimeoutExpired:
            test_run.result = TestResult.ERROR
            test_run.error = "Test execution timed out"
            print(f"â±ï¸  {category.value.upper()} ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        
        except Exception as e:
            test_run.result = TestResult.ERROR
            test_run.error = str(e)
            print(f"ğŸ’¥ {category.value.upper()} ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        test_run.end_time = time.time()
        test_run.duration = test_run.end_time - test_run.start_time
        
        return test_run
    
    def _parse_json_report(self, test_run: TestRun, json_file: Path):
        """JSON ãƒ¬ãƒãƒ¼ãƒˆè§£æ"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            summary = report.get('summary', {})
            test_run.tests_passed = summary.get('passed', 0)
            test_run.tests_failed = summary.get('failed', 0)
            test_run.tests_skipped = summary.get('skipped', 0)
            test_run.tests_total = summary.get('total', 0)
            
            # Extract coverage if available
            if 'coverage' in report:
                test_run.coverage = report['coverage'].get('percent_covered')
        
        except Exception as e:
            print(f"âš ï¸  JSON ãƒ¬ãƒãƒ¼ãƒˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _print_summary(self, results: List[TestRun]):
        """ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        
        total_duration = sum(r.duration for r in results)
        total_tests = sum(r.tests_total for r in results)
        total_passed = sum(r.tests_passed for r in results)
        total_failed = sum(r.tests_failed for r in results)
        total_skipped = sum(r.tests_skipped for r in results)
        
        print(f"ğŸ“ ç·å®Ÿè¡Œæ™‚é–“: {total_duration:.1f}ç§’")
        print(f"ğŸ¯ ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
        print(f"âœ… æˆåŠŸ: {total_passed}")
        print(f"âŒ å¤±æ•—: {total_failed}")
        print(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {total_skipped}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(total_passed/total_tests*100):.1f}%" if total_tests > 0 else "N/A")
        
        print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°:")
        for result in results:
            status_icon = {
                TestResult.PASSED: "âœ…",
                TestResult.FAILED: "âŒ", 
                TestResult.SKIPPED: "â­ï¸",
                TestResult.ERROR: "ğŸ’¥"
            }.get(result.result, "â“")
            
            coverage_info = f" (ã‚«ãƒãƒ¬ãƒƒã‚¸: {result.coverage:.1f}%)" if result.coverage else ""
            
            print(f"  {status_icon} {result.category.value.upper()}: "
                  f"{result.tests_passed}/{result.tests_total} "
                  f"({result.duration:.1f}ç§’){coverage_info}")
        
        # Overall result
        print(f"\nğŸ å…¨ä½“çµæœ: ", end="")
        if all(r.result == TestResult.PASSED for r in results):
            print("âœ… å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ")
        elif any(r.result == TestResult.FAILED for r in results):
            print("âŒ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•—")
        else:
            print("âš ï¸  ãƒ†ã‚¹ãƒˆå®Œäº†ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã‚ã‚Šï¼‰")
    
    def run_specific_test(self, test_path: str, verbose: bool = True) -> TestRun:
        """ç‰¹å®šãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        test_run = TestRun(category=TestCategory.UNIT, start_time=time.time())
        
        cmd = ["python", "-m", "pytest", test_path]
        if verbose:
            cmd.append("-v")
        
        try:
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True
            )
            
            test_run.exit_code = result.returncode
            test_run.output = result.stdout
            test_run.error = result.stderr
            test_run.result = TestResult.PASSED if result.returncode == 0 else TestResult.FAILED
            
        except Exception as e:
            test_run.result = TestResult.ERROR
            test_run.error = str(e)
        
        test_run.end_time = time.time()
        test_run.duration = test_run.end_time - test_run.start_time
        
        return test_run
    
    def generate_html_report(self, output_path: Path):
        """HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        html_content = """
<!DOCTYPE html>
<html>
<head>
    <title>Test Results Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .header { background-color: #f0f0f0; padding: 20px; border-radius: 5px; }
        .summary { margin: 20px 0; }
        .test-category { margin: 10px 0; padding: 10px; border-left: 4px solid #ccc; }
        .passed { border-left-color: #28a745; }
        .failed { border-left-color: #dc3545; }
        .skipped { border-left-color: #ffc107; }
        .error { border-left-color: #fd7e14; }
        .metrics { display: flex; gap: 20px; margin: 20px 0; }
        .metric { background: #f8f9fa; padding: 15px; border-radius: 5px; text-align: center; }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ§ª Backend Test Results</h1>
        <p>Generated at: {timestamp}</p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“Š Overall Summary</h2>
        <div class="metrics">
            <div class="metric">
                <h3>{total_tests}</h3>
                <p>Total Tests</p>
            </div>
            <div class="metric">
                <h3>{total_passed}</h3>
                <p>Passed</p>
            </div>
            <div class="metric">
                <h3>{total_failed}</h3>
                <p>Failed</p>
            </div>
            <div class="metric">
                <h3>{success_rate:.1f}%</h3>
                <p>Success Rate</p>
            </div>
        </div>
    </div>
    
    <div class="details">
        <h2>ğŸ“‹ Category Details</h2>
        {category_details}
    </div>
</body>
</html>
"""
        
        # Calculate totals
        total_tests = sum(r.tests_total for r in self.test_results)
        total_passed = sum(r.tests_passed for r in self.test_results)
        total_failed = sum(r.tests_failed for r in self.test_results)
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        # Generate category details
        category_html = ""
        for result in self.test_results:
            status_class = result.result.value if result.result else "unknown"
            coverage_info = f"Coverage: {result.coverage:.1f}%" if result.coverage else "Coverage: N/A"
            
            category_html += f"""
            <div class="test-category {status_class}">
                <h3>{result.category.value.upper()}</h3>
                <p><strong>Status:</strong> {result.result.value if result.result else "Unknown"}</p>
                <p><strong>Tests:</strong> {result.tests_passed}/{result.tests_total}</p>
                <p><strong>Duration:</strong> {result.duration:.1f}s</p>
                <p><strong>{coverage_info}</strong></p>
            </div>
            """
        
        # Fill template
        final_html = html_content.format(
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            total_tests=total_tests,
            total_passed=total_passed,
            total_failed=total_failed,
            success_rate=success_rate,
            category_details=category_html
        )
        
        # Write file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(final_html)
        
        print(f"ğŸ“„ HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {output_path}")
    
    def export_results_json(self, output_path: Path):
        """çµæœã‚’JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        export_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "summary": {
                "total_tests": sum(r.tests_total for r in self.test_results),
                "total_passed": sum(r.tests_passed for r in self.test_results),
                "total_failed": sum(r.tests_failed for r in self.test_results),
                "total_skipped": sum(r.tests_skipped for r in self.test_results),
                "total_duration": sum(r.duration for r in self.test_results)
            },
            "categories": []
        }
        
        for result in self.test_results:
            export_data["categories"].append({
                "category": result.category.value,
                "result": result.result.value if result.result else None,
                "tests_total": result.tests_total,
                "tests_passed": result.tests_passed,
                "tests_failed": result.tests_failed,
                "tests_skipped": result.tests_skipped,
                "duration": result.duration,
                "coverage": result.coverage,
                "exit_code": result.exit_code
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ JSON ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {output_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼")
    
    parser.add_argument(
        "--categories", "-c",
        nargs="+",
        choices=[cat.value for cat in TestCategory],
        default=["all"],
        help="å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒª"
    )
    
    parser.add_argument(
        "--no-coverage",
        action="store_true",
        help="ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šã‚’ç„¡åŠ¹åŒ–"
    )
    
    parser.add_argument(
        "--parallel", "-p",
        action="store_true",
        help="ä¸¦åˆ—å®Ÿè¡Œã‚’æœ‰åŠ¹åŒ–"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="è©³ç´°å‡ºåŠ›"
    )
    
    parser.add_argument(
        "--fail-fast", "-x",
        action="store_true",
        help="æœ€åˆã®å¤±æ•—ã§åœæ­¢"
    )
    
    parser.add_argument(
        "--output-dir", "-o",
        type=Path,
        default=Path("test_results"),
        help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    
    parser.add_argument(
        "--test-file",
        help="ç‰¹å®šã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œ"
    )
    
    args = parser.parse_args()
    
    # Create output directory
    args.output_dir.mkdir(exist_ok=True)
    
    runner = TestRunner()
    
    if args.test_file:
        # Run specific test
        result = runner.run_specific_test(args.test_file, args.verbose)
        print(f"Test result: {result.result.value if result.result else 'Unknown'}")
        return result.exit_code
    
    # Parse categories
    categories = [TestCategory(cat) for cat in args.categories]
    
    # Run tests
    results = runner.run_tests(
        categories=categories,
        coverage=not args.no_coverage,
        parallel=args.parallel,
        verbose=args.verbose,
        fail_fast=args.fail_fast
    )
    
    # Generate reports
    runner.generate_html_report(args.output_dir / "test_report.html")
    runner.export_results_json(args.output_dir / "test_results.json")
    
    # Return appropriate exit code
    if any(r.result == TestResult.FAILED for r in results):
        return 1
    elif any(r.result == TestResult.ERROR for r in results):
        return 2
    else:
        return 0


if __name__ == "__main__":
    sys.exit(main())