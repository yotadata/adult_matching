"""
ML Integration Tests Configuration

MLçµ±åˆãƒ†ã‚¹ãƒˆè¨­å®š
å…±é€šãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã¨ãƒ†ã‚¹ãƒˆç’°å¢ƒè¨­å®š
"""

import pytest
import os
import tempfile
import shutil
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List
from unittest.mock import Mock, patch

from backend.ml.config import TrainingConfig
from backend.ml.preprocessing.features.feature_processor import FeatureConfig


@pytest.fixture(scope="session")
def ml_test_environment():
    """MLçµ±åˆãƒ†ã‚¹ãƒˆç’°å¢ƒï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ï¼‰"""
    # ãƒ†ã‚¹ãƒˆç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    base_temp_dir = tempfile.mkdtemp(prefix="ml_integration_test_session_")

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ
    directories = [
        "models", "data", "artifacts", "logs", "exports",
        "preprocessors", "features", "embeddings", "checkpoints",
        "tensorboard", "metrics", "deployments"
    ]

    env = {"base_dir": Path(base_temp_dir)}

    for dir_name in directories:
        dir_path = Path(base_temp_dir) / dir_name
        dir_path.mkdir(exist_ok=True)
        env[f"{dir_name}_dir"] = dir_path

    # ç’°å¢ƒå¤‰æ•°è¨­å®š
    os.environ['ML_TEST_MODE'] = 'true'
    os.environ['ML_TEST_BASE_DIR'] = str(base_temp_dir)

    yield env

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    shutil.rmtree(base_temp_dir, ignore_errors=True)
    os.environ.pop('ML_TEST_MODE', None)
    os.environ.pop('ML_TEST_BASE_DIR', None)


@pytest.fixture
def standard_config():
    """æ¨™æº–ãƒ†ã‚¹ãƒˆè¨­å®š"""
    return TrainingConfig(
        user_embedding_dim=768,
        item_embedding_dim=768,
        user_hidden_units=[512, 256, 128],
        item_hidden_units=[512, 256, 128],
        batch_size=64,
        epochs=3,
        learning_rate=0.001,
        validation_split=0.2,
        early_stopping_patience=2,
        save_best_only=True,
        reduce_lr_on_plateau=True
    )


@pytest.fixture
def fast_test_config():
    """é«˜é€Ÿãƒ†ã‚¹ãƒˆç”¨è¨­å®šï¼ˆé–‹ç™ºç”¨ï¼‰"""
    return TrainingConfig(
        user_embedding_dim=64,
        item_embedding_dim=64,
        user_hidden_units=[32, 16],
        item_hidden_units=[32, 16],
        batch_size=32,
        epochs=1,
        learning_rate=0.01,
        validation_split=0.1
    )


@pytest.fixture
def feature_test_config():
    """ç‰¹å¾´é‡å‡¦ç†ãƒ†ã‚¹ãƒˆç”¨è¨­å®š"""
    return FeatureConfig(
        target_dimension=768,
        user_features=[
            'age', 'preferences', 'activity_level',
            'registration_date', 'premium_user'
        ],
        item_features=[
            'genre', 'duration', 'quality_score',
            'popularity_score', 'features', 'tags'
        ],
        categorical_features=[
            'genre', 'age_group', 'user_type'
        ],
        numerical_features=[
            'age', 'duration', 'quality_score',
            'activity_level', 'popularity_score'
        ],
        embedding_features=[
            'preferences', 'features', 'tags'
        ]
    )


@pytest.fixture
def sample_small_dataset():
    """å°è¦æ¨¡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    np.random.seed(42)

    users = pd.DataFrame([
        {
            'user_id': f'user_{i}',
            'age': np.random.randint(18, 60),
            'preferences': np.random.random(10).tolist(),
            'activity_level': np.random.random(),
            'registration_date': f"2024-{np.random.randint(1, 13):02d}-01",
            'premium_user': np.random.choice([True, False], p=[0.3, 0.7])
        }
        for i in range(50)
    ])

    items = pd.DataFrame([
        {
            'item_id': f'video_{i}',
            'genre': np.random.choice(['action', 'romance', 'comedy', 'drama']),
            'duration': np.random.randint(60, 180),
            'quality_score': np.random.random(),
            'popularity_score': np.random.random(),
            'features': np.random.random(15).tolist(),
            'tags': np.random.choice(['tag1', 'tag2', 'tag3'], size=3).tolist()
        }
        for i in range(100)
    ])

    interactions = pd.DataFrame([
        {
            'user_id': f'user_{np.random.randint(0, 50)}',
            'item_id': f'video_{np.random.randint(0, 100)}',
            'rating': np.random.choice([0, 1], p=[0.3, 0.7]),
            'timestamp': f"2024-01-{np.random.randint(1, 30):02d}",
            'interaction_type': np.random.choice(['like', 'view', 'share'])
        }
        for i in range(500)
    ])

    return {
        'users': users,
        'items': items,
        'interactions': interactions
    }


@pytest.fixture
def sample_medium_dataset():
    """ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    np.random.seed(42)

    users = pd.DataFrame([
        {
            'user_id': f'user_{i}',
            'age': np.random.randint(18, 65),
            'preferences': np.random.random(15).tolist(),
            'activity_level': np.random.random(),
            'registration_date': f"2024-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}",
            'premium_user': np.random.choice([True, False], p=[0.25, 0.75])
        }
        for i in range(1000)
    ])

    items = pd.DataFrame([
        {
            'item_id': f'video_{i}',
            'genre': np.random.choice(['action', 'romance', 'comedy', 'drama', 'thriller', 'sci-fi']),
            'duration': np.random.randint(30, 240),
            'quality_score': np.random.random(),
            'popularity_score': np.random.random(),
            'features': np.random.random(20).tolist(),
            'tags': np.random.choice(['tag1', 'tag2', 'tag3', 'tag4', 'tag5'], size=4).tolist()
        }
        for i in range(5000)
    ])

    interactions = pd.DataFrame([
        {
            'user_id': f'user_{np.random.randint(0, 1000)}',
            'item_id': f'video_{np.random.randint(0, 5000)}',
            'rating': np.random.choice([0, 1], p=[0.2, 0.8]),
            'timestamp': f"2024-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}",
            'interaction_type': np.random.choice(['like', 'view', 'share', 'download'])
        }
        for i in range(50000)
    ])

    return {
        'users': users,
        'items': items,
        'interactions': interactions
    }


@pytest.fixture
def mock_database():
    """ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"""
    class MockDatabase:
        def __init__(self):
            self.data = {}

        def insert(self, table: str, data: Dict[str, Any]) -> bool:
            if table not in self.data:
                self.data[table] = []
            self.data[table].append(data)
            return True

        def select(self, table: str, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
            if table not in self.data:
                return []

            result = self.data[table]

            if filters:
                for key, value in filters.items():
                    result = [row for row in result if row.get(key) == value]

            return result

        def update(self, table: str, filters: Dict[str, Any], updates: Dict[str, Any]) -> bool:
            if table not in self.data:
                return False

            for row in self.data[table]:
                match = True
                for key, value in filters.items():
                    if row.get(key) != value:
                        match = False
                        break

                if match:
                    row.update(updates)

            return True

        def delete(self, table: str, filters: Dict[str, Any]) -> bool:
            if table not in self.data:
                return False

            original_count = len(self.data[table])
            self.data[table] = [
                row for row in self.data[table]
                if not all(row.get(k) == v for k, v in filters.items())
            ]

            return len(self.data[table]) < original_count

    return MockDatabase()


@pytest.fixture
def mock_supabase_client():
    """ãƒ¢ãƒƒã‚¯Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    class MockSupabaseClient:
        def __init__(self):
            self.data = {}

        def table(self, table_name: str):
            return MockTable(table_name, self.data)

    class MockTable:
        def __init__(self, table_name: str, data_storage: Dict):
            self.table_name = table_name
            self.data_storage = data_storage

        def select(self, columns: str = "*"):
            return MockQuery(self.table_name, self.data_storage)

        def insert(self, data: Dict[str, Any]):
            if self.table_name not in self.data_storage:
                self.data_storage[self.table_name] = []
            self.data_storage[self.table_name].append(data)
            return MockResponse([data])

        def update(self, data: Dict[str, Any]):
            return MockQuery(self.table_name, self.data_storage, update_data=data)

        def delete(self):
            return MockQuery(self.table_name, self.data_storage, delete_mode=True)

    class MockQuery:
        def __init__(self, table_name: str, data_storage: Dict, update_data: Dict = None, delete_mode: bool = False):
            self.table_name = table_name
            self.data_storage = data_storage
            self.update_data = update_data
            self.delete_mode = delete_mode
            self.filters = {}

        def eq(self, column: str, value: Any):
            self.filters[column] = value
            return self

        def limit(self, count: int):
            self.limit_count = count
            return self

        def execute(self):
            if self.table_name not in self.data_storage:
                return MockResponse([])

            data = self.data_storage[self.table_name]

            # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
            if self.filters:
                data = [
                    row for row in data
                    if all(row.get(k) == v for k, v in self.filters.items())
                ]

            # æ“ä½œå®Ÿè¡Œ
            if self.delete_mode:
                self.data_storage[self.table_name] = [
                    row for row in self.data_storage[self.table_name]
                    if not all(row.get(k) == v for k, v in self.filters.items())
                ]
                return MockResponse([])

            if self.update_data:
                for row in self.data_storage[self.table_name]:
                    if all(row.get(k) == v for k, v in self.filters.items()):
                        row.update(self.update_data)
                return MockResponse(data)

            # åˆ¶é™é©ç”¨
            if hasattr(self, 'limit_count'):
                data = data[:self.limit_count]

            return MockResponse(data)

    class MockResponse:
        def __init__(self, data: List[Dict[str, Any]]):
            self.data = data

    return MockSupabaseClient()


@pytest.fixture(autouse=True)
def ml_test_patches():
    """MLçµ±åˆãƒ†ã‚¹ãƒˆç”¨ãƒ‘ãƒƒãƒï¼ˆè‡ªå‹•é©ç”¨ï¼‰"""
    patches = []

    # é•·æ™‚é–“å‡¦ç†ã®ãƒ¢ãƒƒã‚¯
    mock_training = Mock(return_value={
        'success': True,
        'final_loss': 0.234,
        'training_time_minutes': 5.0,
        'epochs_completed': 3,
        'training_history': [
            {'epoch': 1, 'loss': 0.456, 'val_loss': 0.478},
            {'epoch': 2, 'loss': 0.345, 'val_loss': 0.367},
            {'epoch': 3, 'loss': 0.234, 'val_loss': 0.248}
        ]
    })

    # å¿…è¦ã«å¿œã˜ã¦ãƒ‘ãƒƒãƒã‚’è¿½åŠ 
    # patches.append(patch('backend.ml.training.trainers.unified_two_tower_trainer.UnifiedTwoTowerTrainer.train', mock_training))

    for p in patches:
        p.start()

    yield

    for p in patches:
        p.stop()


# ã‚«ã‚¹ã‚¿ãƒ ãƒžãƒ¼ã‚«ãƒ¼
def pytest_configure(config):
    """ã‚«ã‚¹ã‚¿ãƒ ãƒžãƒ¼ã‚«ãƒ¼è¨­å®š"""
    config.addinivalue_line("markers", "ml: ML related tests")
    config.addinivalue_line("markers", "integration: Integration tests")
    config.addinivalue_line("markers", "performance: Performance tests")
    config.addinivalue_line("markers", "slow: Slow running tests")
    config.addinivalue_line("markers", "requires_gpu: Tests requiring GPU")
    config.addinivalue_line("markers", "requires_model: Tests requiring trained model")


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå‰ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
def pytest_sessionstart(session):
    """ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚å‡¦ç†"""
    print("\nðŸš€ Starting ML Integration Test Session...")
    print("Setting up test environment...")


def pytest_sessionfinish(session, exitstatus):
    """ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚å‡¦ç†"""
    print("\nâœ… ML Integration Test Session Complete")
    print(f"Exit status: {exitstatus}")


# ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ¯Žã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
def pytest_runtest_setup(item):
    """å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå‰å‡¦ç†"""
    if item.get_closest_marker("requires_gpu"):
        # GPUè¦æ±‚ãƒ†ã‚¹ãƒˆã®ãƒã‚§ãƒƒã‚¯
        try:
            import tensorflow as tf
            if not tf.config.list_physical_devices('GPU'):
                pytest.skip("GPU not available")
        except ImportError:
            pytest.skip("TensorFlow not available")

    if item.get_closest_marker("slow"):
        # ç’°å¢ƒå¤‰æ•°ã§ã‚¹ãƒ­ãƒ¼ ãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—
        if os.environ.get("SKIP_SLOW_TESTS", "false").lower() == "true":
            pytest.skip("Slow test skipped")


# ãƒ†ã‚¹ãƒˆçµæžœãƒ¬ãƒãƒ¼ãƒˆ
def pytest_runtest_logreport(report):
    """ãƒ†ã‚¹ãƒˆçµæžœãƒ¬ãƒãƒ¼ãƒˆ"""
    if report.when == "call":
        if report.passed:
            print(f"âœ… {report.nodeid}")
        elif report.failed:
            print(f"âŒ {report.nodeid}")
        elif report.skipped:
            print(f"â­ï¸  {report.nodeid}")