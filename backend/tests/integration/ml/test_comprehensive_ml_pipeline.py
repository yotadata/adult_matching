"""
Comprehensive ML Pipeline Integration Tests

åŒ…æ‹¬çš„MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆ
MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ
"""

import pytest
import tempfile
import shutil
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List
import os
import asyncio
from unittest.mock import Mock, patch

from backend.ml.config import TrainingConfig
from backend.ml.training.trainers.unified_two_tower_trainer import UnifiedTwoTowerTrainer
from backend.ml.preprocessing.features.feature_processor import FeatureProcessor, FeatureConfig
from backend.ml.evaluation.metrics.model_evaluator import ModelEvaluator
from backend.ml.deployment.model_converter import ModelConverter
from backend.ml.inference.realtime.realtime_predictor import RealtimePredictor
from backend.data.unified_data_manager import UnifiedDataManager


@pytest.fixture
def comprehensive_config():
    """åŒ…æ‹¬ãƒ†ã‚¹ãƒˆç”¨è¨­å®š"""
    return TrainingConfig(
        user_embedding_dim=768,
        item_embedding_dim=768,
        user_hidden_units=[512, 256, 128],
        item_hidden_units=[512, 256, 128],
        batch_size=64,
        epochs=3,
        learning_rate=0.001,
        validation_split=0.2,
        early_stopping_patience=2
    )


@pytest.fixture
def test_environment():
    """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    temp_dir = tempfile.mkdtemp(prefix="ml_comprehensive_test_")

    # ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ
    dirs = [
        "models", "data", "artifacts", "logs", "exports",
        "preprocessors", "features", "embeddings"
    ]

    for dir_name in dirs:
        (Path(temp_dir) / dir_name).mkdir(exist_ok=True)

    yield {
        "temp_dir": temp_dir,
        "models_dir": Path(temp_dir) / "models",
        "data_dir": Path(temp_dir) / "data",
        "artifacts_dir": Path(temp_dir) / "artifacts",
        "logs_dir": Path(temp_dir) / "logs",
        "exports_dir": Path(temp_dir) / "exports"
    }

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    shutil.rmtree(temp_dir, ignore_errors=True)


@pytest.fixture
def sample_training_data():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿"""
    np.random.seed(42)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å¾´é‡ï¼ˆ100ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
    users = []
    for i in range(100):
        users.append({
            'user_id': f'user_{i}',
            'age': np.random.randint(18, 60),
            'preferences': np.random.random(10).tolist(),
            'activity_level': np.random.random()
        })

    # ã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡ï¼ˆ200å‹•ç”»ï¼‰
    items = []
    for i in range(200):
        items.append({
            'item_id': f'video_{i}',
            'genre': np.random.choice(['action', 'romance', 'comedy', 'drama']),
            'duration': np.random.randint(60, 180),
            'quality_score': np.random.random(),
            'features': np.random.random(20).tolist()
        })

    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ2000ä»¶ï¼‰
    interactions = []
    for i in range(2000):
        user_id = f"user_{np.random.randint(0, 100)}"
        item_id = f"video_{np.random.randint(0, 200)}"
        rating = np.random.choice([0, 1], p=[0.3, 0.7])  # 70%ãŒæ­£ä¾‹

        interactions.append({
            'user_id': user_id,
            'item_id': item_id,
            'rating': rating,
            'timestamp': f"2024-01-{np.random.randint(1, 30):02d}"
        })

    return {
        'users': users,
        'items': items,
        'interactions': interactions
    }


class TestComprehensiveMLPipeline:
    """åŒ…æ‹¬çš„MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆ"""

    @pytest.mark.integration
    @pytest.mark.ml
    @pytest.mark.slow
    async def test_complete_ml_workflow(self, comprehensive_config, test_environment, sample_training_data):
        """å®Œå…¨MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        print("ğŸš€ Starting complete ML workflow test...")

        # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨å‰å‡¦ç†
        print("ğŸ“Š Phase 1: Data preprocessing...")

        feature_config = FeatureConfig(
            target_dimension=768,
            user_features=['age', 'preferences', 'activity_level'],
            item_features=['genre', 'duration', 'quality_score', 'features']
        )

        feature_processor = FeatureProcessor(feature_config)

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        users_df = pd.DataFrame(sample_training_data['users'])
        items_df = pd.DataFrame(sample_training_data['items'])
        interactions_df = pd.DataFrame(sample_training_data['interactions'])

        # ç‰¹å¾´é‡å‡¦ç†
        feature_processor.fit(users_df, items_df, interactions_df)

        user_features = feature_processor.transform_users(users_df)
        item_features = feature_processor.transform_items(items_df)

        assert user_features.shape[1] == 768, "User features dimension mismatch"
        assert item_features.shape[1] == 768, "Item features dimension mismatch"
        assert len(user_features) == 100, "User count mismatch"
        assert len(item_features) == 200, "Item count mismatch"

        print(f"âœ… Features processed: users {user_features.shape}, items {item_features.shape}")

        # 2. ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        print("ğŸ¯ Phase 2: Model training...")

        trainer = UnifiedTwoTowerTrainer(
            config=comprehensive_config,
            model_save_dir=str(test_environment["models_dir"]),
            experiment_name="comprehensive_test"
        )

        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™
        training_data = {
            'user_features': user_features,
            'item_features': item_features,
            'interactions': interactions_df
        }

        # ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        with patch.object(trainer, '_save_model') as mock_save:
            mock_save.return_value = True

            training_results = await asyncio.to_thread(
                trainer.train, training_data
            )

        assert training_results['success'] == True
        assert training_results['final_loss'] > 0
        assert 'training_history' in training_results
        assert len(training_results['training_history']) > 0

        print(f"âœ… Training completed: loss {training_results['final_loss']:.4f}")

        # 3. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        print("ğŸ“ˆ Phase 3: Model evaluation...")

        evaluator = ModelEvaluator()

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_interactions = interactions_df.sample(n=400, random_state=42)
        true_labels = test_interactions['rating'].values

        # äºˆæ¸¬ç”Ÿæˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        with patch.object(trainer, 'predict') as mock_predict:
            mock_predict.return_value = np.random.random(len(true_labels))
            predictions = trainer.predict(test_interactions)

        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        metrics = evaluator.calculate_metrics(true_labels, predictions)
        ranking_metrics = evaluator.calculate_ranking_metrics(true_labels, predictions)

        # æŒ‡æ¨™æ¤œè¨¼
        required_metrics = ['auc_roc', 'auc_pr', 'accuracy', 'precision', 'recall', 'f1_score']
        for metric in required_metrics:
            assert metric in metrics
            assert 0 <= metrics[metric] <= 1

        assert metrics['auc_pr'] >= 0.85, f"AUC-PR {metrics['auc_pr']:.3f} below requirement (â‰¥0.85)"

        print(f"âœ… Evaluation completed: AUC-PR {metrics['auc_pr']:.3f}")

        # 4. ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
        print("ğŸš€ Phase 4: Model deployment...")

        converter = ModelConverter()

        # TensorFlow.jså¤‰æ›ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        with patch.object(converter, 'to_tensorflowjs') as mock_tfjs:
            mock_tfjs.return_value = {
                'success': True,
                'output_path': str(test_environment["exports_dir"] / "tfjs_model"),
                'model_size_mb': 12.5
            }

            tfjs_result = converter.to_tensorflowjs(
                trainer=trainer,
                output_path=str(test_environment["exports_dir"] / "tfjs_model")
            )

        assert tfjs_result['success'] == True
        assert tfjs_result['model_size_mb'] < 50  # ã‚µã‚¤ã‚ºåˆ¶é™

        # ONNXå¤‰æ›ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        with patch.object(converter, 'to_onnx') as mock_onnx:
            mock_onnx.return_value = {
                'success': True,
                'output_path': str(test_environment["exports_dir"] / "model.onnx"),
                'optimization_level': 'all'
            }

            onnx_result = converter.to_onnx(
                trainer=trainer,
                output_path=str(test_environment["exports_dir"] / "model.onnx")
            )

        assert onnx_result['success'] == True

        print("âœ… Model deployment completed")

        # 5. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ãƒ†ã‚¹ãƒˆ
        print("âš¡ Phase 5: Real-time inference...")

        predictor = RealtimePredictor(
            model_path=str(test_environment["exports_dir"] / "tfjs_model")
        )

        # æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        with patch.object(predictor, 'predict') as mock_inference:
            mock_inference.return_value = {
                'predictions': np.random.random(10).tolist(),
                'inference_time_ms': 45.2,
                'model_version': 'v1.0.0'
            }

            inference_result = predictor.predict(
                user_id='user_1',
                candidate_items=['video_1', 'video_2', 'video_3', 'video_4', 'video_5',
                               'video_6', 'video_7', 'video_8', 'video_9', 'video_10']
            )

        assert len(inference_result['predictions']) == 10
        assert inference_result['inference_time_ms'] < 500  # <500msè¦ä»¶

        print(f"âœ… Inference completed: {inference_result['inference_time_ms']:.1f}ms")

        # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼
        print("ğŸ” Phase 6: Performance validation...")

        performance_metrics = {
            'training_time': training_results.get('training_time_minutes', 0),
            'model_accuracy': metrics['auc_pr'],
            'inference_latency': inference_result['inference_time_ms'],
            'model_size': tfjs_result['model_size_mb']
        }

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶æ¤œè¨¼
        assert performance_metrics['training_time'] < 120, "Training time exceeds 2 hours"
        assert performance_metrics['model_accuracy'] >= 0.85, "Model accuracy below threshold"
        assert performance_metrics['inference_latency'] < 500, "Inference latency exceeds 500ms"
        assert performance_metrics['model_size'] < 50, "Model size exceeds 50MB"

        print("âœ… Performance validation passed")

        # æœ€çµ‚çµæœ
        final_results = {
            'workflow_status': 'success',
            'stages_completed': [
                'data_preprocessing', 'model_training', 'model_evaluation',
                'model_deployment', 'realtime_inference', 'performance_validation'
            ],
            'performance_metrics': performance_metrics,
            'model_metrics': metrics,
            'test_summary': {
                'total_users': len(users_df),
                'total_items': len(items_df),
                'total_interactions': len(interactions_df),
                'test_interactions': len(test_interactions)
            }
        }

        print("ğŸ‰ Complete ML workflow test PASSED")
        return final_results

    @pytest.mark.integration
    @pytest.mark.ml
    def test_ml_pipeline_error_handling(self, comprehensive_config, test_environment):
        """MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ› ï¸ Testing ML pipeline error handling...")

        # 1. ä¸æ­£ãƒ‡ãƒ¼ã‚¿ã§ã®å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼
        feature_config = FeatureConfig(target_dimension=768)
        feature_processor = FeatureProcessor(feature_config)

        # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        empty_df = pd.DataFrame()

        with pytest.raises(Exception):
            feature_processor.fit(empty_df, empty_df, empty_df)

        # 2. ä¸æ­£è¨­å®šã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼
        invalid_config = TrainingConfig(
            user_embedding_dim=-1,  # ç„¡åŠ¹ãªæ¬¡å…ƒ
            epochs=0  # ç„¡åŠ¹ãªã‚¨ãƒãƒƒã‚¯æ•°
        )

        with pytest.raises(ValueError):
            UnifiedTwoTowerTrainer(
                config=invalid_config,
                model_save_dir=str(test_environment["models_dir"])
            )

        # 3. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¨ãƒ©ãƒ¼
        evaluator = ModelEvaluator()

        # ä¸é©åˆ‡ãªã‚µã‚¤ã‚ºã®å…¥åŠ›
        with pytest.raises(Exception):
            evaluator.calculate_metrics([1, 0, 1], [0.5])  # ã‚µã‚¤ã‚ºä¸ä¸€è‡´

        print("âœ… Error handling test passed")

    @pytest.mark.integration
    @pytest.mark.ml
    async def test_ml_pipeline_scalability(self, comprehensive_config, test_environment):
        """MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ“ˆ Testing ML pipeline scalability...")

        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
        large_dataset_sizes = [1000, 5000, 10000]  # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°

        results = []

        for user_count in large_dataset_sizes:
            print(f"Testing with {user_count} users...")

            # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            users_df = pd.DataFrame([
                {
                    'user_id': f'user_{i}',
                    'age': np.random.randint(18, 60),
                    'preferences': np.random.random(10).tolist()
                }
                for i in range(user_count)
            ])

            items_df = pd.DataFrame([
                {
                    'item_id': f'video_{i}',
                    'genre': np.random.choice(['action', 'romance']),
                    'features': np.random.random(20).tolist()
                }
                for i in range(user_count * 2)  # ã‚¢ã‚¤ãƒ†ãƒ æ•°ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®2å€
            ])

            # å‡¦ç†æ™‚é–“æ¸¬å®š
            start_time = pd.Timestamp.now()

            feature_config = FeatureConfig(target_dimension=768)
            feature_processor = FeatureProcessor(feature_config)

            try:
                feature_processor.fit(users_df, items_df, pd.DataFrame())
                user_features = feature_processor.transform_users(users_df)
                item_features = feature_processor.transform_items(items_df)

                end_time = pd.Timestamp.now()
                processing_time = (end_time - start_time).total_seconds()

                results.append({
                    'user_count': user_count,
                    'item_count': len(items_df),
                    'processing_time_seconds': processing_time,
                    'user_features_shape': user_features.shape,
                    'item_features_shape': item_features.shape,
                    'success': True
                })

                print(f"âœ… {user_count} users processed in {processing_time:.2f}s")

                # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ¶é™ãƒã‚§ãƒƒã‚¯
                assert processing_time < 300, f"Processing time {processing_time}s exceeds 5min limit"

            except Exception as e:
                results.append({
                    'user_count': user_count,
                    'success': False,
                    'error': str(e)
                })
                print(f"âŒ Failed with {user_count} users: {e}")

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ
        successful_results = [r for r in results if r['success']]
        assert len(successful_results) >= 2, "Pipeline failed scalability test"

        # ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç¢ºèªï¼ˆå¤§ã¾ã‹ãªï¼‰
        if len(successful_results) >= 2:
            time_per_user = [r['processing_time_seconds'] / r['user_count'] for r in successful_results]
            avg_time_per_user = np.mean(time_per_user)

            assert avg_time_per_user < 0.1, f"Processing time per user {avg_time_per_user:.4f}s too high"

        print("âœ… Scalability test passed")
        return results

    @pytest.mark.integration
    @pytest.mark.ml
    def test_ml_monitoring_integration(self, comprehensive_config, test_environment):
        """MLç›£è¦–çµ±åˆãƒ†ã‚¹ãƒˆ"""
        print("ğŸ“Š Testing ML monitoring integration...")

        from backend.ml.monitoring.performance.performance_monitor import MLPerformanceMonitor
        from backend.ml.monitoring.quality.quality_monitor import MLQualityMonitor

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        perf_monitor = MLPerformanceMonitor()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        perf_monitor.record_training_metrics({
            'loss': 0.234,
            'accuracy': 0.876,
            'training_time': 1800,
            'memory_usage_mb': 2048
        })

        perf_monitor.record_inference_metrics({
            'latency_ms': 45.2,
            'throughput_qps': 22.1,
            'model_version': 'v1.0.0'
        })

        # ç›£è¦–ãƒ‡ãƒ¼ã‚¿å–å¾—
        training_stats = perf_monitor.get_training_statistics()
        inference_stats = perf_monitor.get_inference_statistics()

        assert len(training_stats) > 0
        assert len(inference_stats) > 0

        # å“è³ªç›£è¦–
        quality_monitor = MLQualityMonitor()

        # ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºãƒ†ã‚¹ãƒˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        with patch.object(quality_monitor, 'detect_data_drift') as mock_drift:
            mock_drift.return_value = {
                'drift_detected': False,
                'drift_score': 0.12,
                'threshold': 0.3
            }

            drift_result = quality_monitor.detect_data_drift(
                reference_data=np.random.random((1000, 50)),
                current_data=np.random.random((100, 50))
            )

        assert 'drift_detected' in drift_result
        assert isinstance(drift_result['drift_score'], float)

        print("âœ… ML monitoring integration passed")

    @pytest.mark.integration
    @pytest.mark.ml
    def test_ml_deployment_validation(self, comprehensive_config, test_environment):
        """MLãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        print("ğŸš€ Testing ML deployment validation...")

        from backend.ml.deployment.model_deployer import ModelDeployer
        from backend.ml.deployment.versioning.version_manager import ModelVersionManager

        deployer = ModelDeployer()
        version_manager = ModelVersionManager(base_path=str(test_environment["artifacts_dir"]))

        # ãƒ€ãƒŸãƒ¼ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
        class MockTrainer:
            def __init__(self):
                self.config = comprehensive_config
                self.model_version = "v1.0.0"

        mock_trainer = MockTrainer()

        # Webé…ä¿¡ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆ
        with patch.object(deployer, 'create_web_package') as mock_web:
            mock_web.return_value = {
                'success': True,
                'package_path': str(test_environment["exports_dir"] / "web_package"),
                'package_size_mb': 15.3,
                'files_included': ['model.json', 'weights.bin', 'inference.js']
            }

            web_result = deployer.create_web_package(
                model_trainer=mock_trainer,
                output_path=str(test_environment["exports_dir"] / "web_package"),
                include_preprocessors=True
            )

        assert web_result['success'] == True
        assert web_result['package_size_mb'] < 50

        # Supabase Edge Functionsé…ä¿¡
        with patch.object(deployer, 'deploy_to_supabase') as mock_supabase:
            mock_supabase.return_value = {
                'success': True,
                'function_name': 'enhanced_two_tower_recommendations',
                'deployment_url': 'https://test.supabase.co/functions/v1/enhanced_two_tower_recommendations'
            }

            supabase_result = deployer.deploy_to_supabase(
                model_trainer=mock_trainer,
                function_name='enhanced_two_tower_recommendations'
            )

        assert supabase_result['success'] == True
        assert 'deployment_url' in supabase_result

        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
        version_result = version_manager.register_version(
            model_trainer=mock_trainer,
            version_tag="v1.0.0",
            description="Comprehensive test deployment",
            metrics={
                'auc_pr': 0.89,
                'accuracy': 0.85,
                'inference_latency_ms': 42.1
            }
        )

        assert version_result['success'] == True

        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¤œç´¢
        versions = version_manager.list_versions()
        assert len(versions) == 1
        assert versions[0]['version'] == "v1.0.0"

        print("âœ… ML deployment validation passed")


if __name__ == "__main__":
    # çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œè¨­å®š
    pytest.main([
        __file__,
        "-v",
        "-s",
        "--tb=short",
        "-m", "integration and ml"
    ])